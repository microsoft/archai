{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training NLP-based Models with Hugging Face\n",
    "\n",
    "Training an NLP-based model involves several steps, including loading the data, encoding the data, defining the model architecture, and conducting the actual training process.\n",
    "\n",
    "Archai implements abstract base classes that defines the expected behavior of some classes, such as datasets (`DatasetProvider`) and trainers (`TrainerBase`). Additionally, we offer boilerplate classes for the most common frameworks, such as a `DatasetProvider` compatible with `huggingface/datasets` and a `TrainerBase` compatible with `huggingface/transformers`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Encoding the Data\n",
    "\n",
    "When using a dataset provider, such as Hugging Face's `datasets` library, the data loading process is simplified, as the provider takes care of downloading and pre-processing the required dataset. Next, the data needs to be encoded, typically by converting text data into numerical representations that can be fed into the model. \n",
    "\n",
    "This step is accomplished in the same way as the [previous notebook](./hf_dataset_provider.ipynb):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikitext (C:/Users/gderosa/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n",
      "Loading cached processed dataset at C:\\Users\\gderosa\\.cache\\huggingface\\datasets\\wikitext\\wikitext-103-raw-v1\\1.0.0\\a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\\cache-04d7ff93d438ade6.arrow\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorForLanguageModeling\n",
    "from archai.datasets.nlp.hf_dataset_provider import HfHubDatasetProvider\n",
    "from archai.datasets.nlp.hf_dataset_provider_utils import tokenize_dataset\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codegen-350M-mono\", model_max_length=1024)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "dataset_provider = HfHubDatasetProvider(\"wikitext\", dataset_config_name=\"wikitext-103-raw-v1\")\n",
    "\n",
    "# When loading `train_dataset`, we will override the split argument to only load 1%\n",
    "# of the data and speed up its encoding\n",
    "train_dataset = dataset_provider.get_train_dataset(split=\"train[:1%]\")\n",
    "encoded_train_dataset = train_dataset.map(tokenize_dataset, batched=True, fn_kwargs={\"tokenizer\": tokenizer})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Model\n",
    "\n",
    "Once the data is encoded, we can define any NLP-based model. In this example, we will use a CodeGen architecture from `huggingface/transformers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CodeGenConfig, CodeGenForCausalLM\n",
    "\n",
    "config = CodeGenConfig(\n",
    "    n_positions=1024,\n",
    "    n_embd=768,\n",
    "    n_layer=12,\n",
    "    n_head=12,\n",
    "    rotary_dim=16,\n",
    "    bos_token_id=0,\n",
    "    eos_token_id=0,\n",
    "    vocab_size=50295,\n",
    ")\n",
    "model = CodeGenForCausalLM(config=config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Trainer\n",
    "\n",
    "The final step is to use the Hugging Face trainer abstraction (`HfTrainer`) to conduct the training process, which involves optimizing the model's parameters using a pre-defined optimization algorithm and loss function, and updating the model's parameters based on the training data. This process is repeated until the model converges to a satisfactory accuracy or performance level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gderosa\\Anaconda3\\envs\\archai\\lib\\site-packages\\transformers\\optimization.py:395: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1316a76337934f1bb3d16645f6f3f469",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CodeGenTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 10.8762, 'learning_rate': 0.0, 'epoch': 0.0}\n",
      "{'train_runtime': 25.7651, 'train_samples_per_second': 0.039, 'train_steps_per_second': 0.039, 'train_loss': 10.876193046569824, 'epoch': 0.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1, training_loss=10.876193046569824, metrics={'train_runtime': 25.7651, 'train_samples_per_second': 0.039, 'train_steps_per_second': 0.039, 'train_loss': 10.876193046569824, 'epoch': 0.0})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "from archai.trainers.nlp.hf_trainer import HfTrainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    \"hf-codegen\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    logging_steps=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    learning_rate=0.01,\n",
    "    weight_decay=0.1,\n",
    "    max_steps=1,\n",
    ")\n",
    "trainer = HfTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collator,\n",
    "    train_dataset=encoded_train_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "archai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b2be478cf8a2d9a6a1293b022e8589530f7ec0d0340a3a36da6068ef3d344086"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
