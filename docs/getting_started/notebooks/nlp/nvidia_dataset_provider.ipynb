{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating NLP-based Data\n",
    "\n",
    "In this notebook, we will use a dataset provider-based abstraction that interfaces with `NVIDIA\n",
    "/DeepLearningExamples` to load and encode pre-defined/custom data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Encoding the Data\n",
    "\n",
    "The first step is to create an instance of the `NvidiaDatasetProvider`, which offers pre-loads the dataset and offers three methods to retrieve them: `get_train_dataset()`, `get_val_dataset()` and `get_test_dataset()`. This is useful when loading pre-defined datasets, as well as loading custom-based (OLX prefix) data, which is composed by raw text files, such as `train.txt`, `valid.txt` and `test.txt`.\n",
    "\n",
    "Additionally, the `NvidiaDatasetProvider` already encodes and caches the data with a built-in tokenizer. One can change the following arguments according to desired needs:\n",
    "\n",
    "* `dataset_dir`: Dataset folder.\n",
    "* `cache_dir`: Path to the cache folder.\n",
    "* `vocab_type`: Type of vocabulary/tokenizer.\n",
    "* `vocab_size`: Vocabulary size.\n",
    "* `refresh_cache`: Whether cache should be refreshed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-21 15:12:37,792 - archai.datasets.nlp.nvidia_dataset_provider_utils — INFO —  Refreshing cache ...\n",
      "2023-03-21 15:12:37,793 - archai.datasets.nlp.nvidia_dataset_provider_utils — INFO —  Clearing and rebuilding cache ...\n",
      "2023-03-21 15:12:37,794 - archai.datasets.nlp.nvidia_dataset_provider_utils — INFO —  Corpus: dataset = olx_tmp | vocab_type = gpt2 | vocab_size = None\n",
      "2023-03-21 15:12:37,796 - archai.datasets.nlp.nvidia_dataset_provider_utils — INFO —  Training vocabulary ...\n",
      "2023-03-21 15:12:37,797 - archai.datasets.nlp.tokenizer_utils.bbpe_tokenizer — INFO —  Training tokenizer with size = 50257 at c:\\Users\\gderosa\\Projects\\archai\\docs\\getting_started\\notebooks\\nlp\\cache\\olx_tmp\\gpt2\\None\\vocab\\bbpe_tokenizer.json ...\n",
      "2023-03-21 15:12:37,798 - archai.datasets.nlp.tokenizer_utils.bbpe_tokenizer — INFO —  Training tokenizer ...\n",
      "2023-03-21 15:12:37,827 - archai.datasets.nlp.tokenizer_utils.bbpe_tokenizer — DEBUG —  Tokenizer length: 264\n",
      "2023-03-21 15:12:37,828 - archai.datasets.nlp.tokenizer_utils.bbpe_tokenizer — DEBUG —  Tokenizer file path: c:\\Users\\gderosa\\Projects\\archai\\docs\\getting_started\\notebooks\\nlp\\cache\\olx_tmp\\gpt2\\None\\vocab\\bbpe_tokenizer.json\n",
      "2023-03-21 15:12:37,830 - archai.datasets.nlp.nvidia_dataset_provider_utils — INFO —  Vocabulary trained.\n",
      "2023-03-21 15:12:37,831 - archai.datasets.nlp.tokenizer_utils.tokenizer_base — INFO —  Encoding file: dataroot/olx_tmp\\train.txt\n",
      "2023-03-21 15:12:37,835 - archai.datasets.nlp.tokenizer_utils.tokenizer_base — INFO —  Encoding file: dataroot/olx_tmp\\valid.txt\n",
      "2023-03-21 15:12:37,841 - archai.datasets.nlp.tokenizer_utils.tokenizer_base — INFO —  Encoding file: dataroot/olx_tmp\\test.txt\n",
      "2023-03-21 15:12:37,843 - archai.datasets.nlp.nvidia_dataset_provider_utils — DEBUG —  Size: train = 7 | valid = 7 | test = 6\n",
      "tensor([200, 222,  85,  83,  66,  74,  79]) tensor([200, 222,  87,  66,  77,  74,  69]) tensor([200, 222,  85,  70,  84,  85])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from archai.datasets.nlp.nvidia_dataset_provider import NvidiaDatasetProvider\n",
    "\n",
    "# In this example, we will create a dummy dataset with 3 splits\n",
    "os.makedirs(\"dataroot/olx_tmp\", exist_ok=True)\n",
    "with open(\"dataroot/olx_tmp/train.txt\", \"w\") as f:\n",
    "    f.write(\"train\")\n",
    "with open(\"dataroot/olx_tmp/valid.txt\", \"w\") as f:\n",
    "    f.write(\"valid\")\n",
    "with open(\"dataroot/olx_tmp/test.txt\", \"w\") as f:\n",
    "    f.write(\"test\")\n",
    "\n",
    "dataset_provider = NvidiaDatasetProvider(\"olx_tmp\", dataset_dir=\"dataroot/olx_tmp\", refresh_cache=True)\n",
    "\n",
    "train_dataset = dataset_provider.get_train_dataset()\n",
    "val_dataset = dataset_provider.get_val_dataset()\n",
    "test_dataset = dataset_provider.get_test_dataset()\n",
    "print(train_dataset, val_dataset, test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "archai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b2be478cf8a2d9a6a1293b022e8589530f7ec0d0340a3a36da6068ef3d344086"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
