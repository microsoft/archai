{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training NLP-based Models with NVIDIA\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Config, GPT2LMHeadModel\n",
    "\n",
    "config = GPT2Config(\n",
    "    vocab_size=50257,\n",
    "    n_positions=16,\n",
    "    n_embd=512,\n",
    "    n_layer=4,\n",
    "    n_head=8,\n",
    "    embd_pdrop=0.0,\n",
    "    attn_pdrop=0.0,\n",
    "    use_cache=False,\n",
    ")\n",
    "model = GPT2LMHeadModel(config=config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-21 15:15:49,613 - archai.datasets.nlp.nvidia_dataset_provider_utils — INFO —  Clearing and rebuilding cache ...\n",
      "2023-03-21 15:15:49,617 - archai.datasets.nlp.nvidia_dataset_provider_utils — INFO —  Corpus: dataset = olx_tmp | vocab_type = gpt2 | vocab_size = None\n",
      "2023-03-21 15:15:49,619 - archai.datasets.nlp.nvidia_dataset_provider_utils — INFO —  Training vocabulary ...\n",
      "2023-03-21 15:15:49,619 - archai.datasets.nlp.tokenizer_utils.bbpe_tokenizer — INFO —  Training tokenizer with size = 50257 at c:\\Users\\gderosa\\Projects\\archai\\docs\\getting_started\\notebooks\\nlp\\dataroot\\textpred\\olx_tmp\\cache\\olx_tmp\\gpt2\\None\\vocab\\bbpe_tokenizer.json ...\n",
      "2023-03-21 15:15:49,619 - archai.datasets.nlp.tokenizer_utils.bbpe_tokenizer — INFO —  Training tokenizer ...\n",
      "2023-03-21 15:15:49,692 - archai.datasets.nlp.tokenizer_utils.bbpe_tokenizer — DEBUG —  Tokenizer length: 264\n",
      "2023-03-21 15:15:49,700 - archai.datasets.nlp.tokenizer_utils.bbpe_tokenizer — DEBUG —  Tokenizer file path: c:\\Users\\gderosa\\Projects\\archai\\docs\\getting_started\\notebooks\\nlp\\dataroot\\textpred\\olx_tmp\\cache\\olx_tmp\\gpt2\\None\\vocab\\bbpe_tokenizer.json\n",
      "2023-03-21 15:15:49,709 - archai.datasets.nlp.nvidia_dataset_provider_utils — INFO —  Vocabulary trained.\n",
      "2023-03-21 15:15:49,713 - archai.datasets.nlp.tokenizer_utils.tokenizer_base — INFO —  Encoding file: c:\\Users\\gderosa\\Projects\\archai\\docs\\getting_started\\notebooks\\nlp\\dataroot\\textpred\\olx_tmp\\train.txt\n",
      "2023-03-21 15:15:49,713 - archai.datasets.nlp.tokenizer_utils.tokenizer_base — INFO —  Encoding file: c:\\Users\\gderosa\\Projects\\archai\\docs\\getting_started\\notebooks\\nlp\\dataroot\\textpred\\olx_tmp\\valid.txt\n",
      "2023-03-21 15:15:49,718 - archai.datasets.nlp.tokenizer_utils.tokenizer_base — INFO —  Encoding file: c:\\Users\\gderosa\\Projects\\archai\\docs\\getting_started\\notebooks\\nlp\\dataroot\\textpred\\olx_tmp\\test.txt\n",
      "2023-03-21 15:15:49,725 - archai.datasets.nlp.nvidia_dataset_provider_utils — DEBUG —  Size: train = 7 | valid = 7 | test = 6\n",
      "2023-03-21 15:15:49,741 - archai.trainers.nlp.nvidia_trainer — INFO —  Starting training ...\n",
      "2023-03-21 15:15:49,747 - archai.trainers.nlp.nvidia_trainer — DEBUG —  Training arguments: {'experiment_name': 'nvidia-gpt2', 'checkpoint_file_path': '', 'output_dir': 'C:\\\\Users\\\\gderosa\\\\logdir\\\\nvidia-gpt2', 'seed': 1234, 'no_cuda': True, 'logging_steps': 1, 'do_eval': False, 'eval_steps': 100, 'save_all_checkpoints': False, 'dataset_name': 'olx_tmp', 'dataset_dir': 'c:\\\\Users\\\\gderosa\\\\Projects\\\\archai\\\\docs\\\\getting_started\\\\notebooks\\\\nlp\\\\dataroot\\\\textpred\\\\olx_tmp', 'dataset_cache_dir': 'c:\\\\Users\\\\gderosa\\\\Projects\\\\archai\\\\docs\\\\getting_started\\\\notebooks\\\\nlp\\\\dataroot\\\\textpred\\\\olx_tmp\\\\cache', 'dataset_refresh_cache': False, 'vocab_type': 'gpt2', 'vocab_size': None, 'iterator_roll': True, 'global_batch_size': 1, 'per_device_global_batch_size': None, 'seq_len': 16, 'strategy': 'dp', 'local_rank': 0, 'find_unused_parameters': False, 'max_steps': 1, 'gradient_accumulation_steps': 1, 'fp16': False, 'optim': 'adam', 'learning_rate': 0.01, 'weight_decay': 0.0, 'momentum': 0.0, 'max_grad_norm': 0.25, 'lr_scheduler_type': 'cosine', 'lr_qat_scheduler_type': 'cosine', 'lr_scheduler_max_steps': None, 'lr_scheduler_warmup_steps': 1000, 'lr_scheduler_patience': 0, 'lr_scheduler_min_lr': 0.001, 'lr_scheduler_decay_rate': 0.5, 'qat': False, 'mixed_qat': False}\n",
      "2023-03-21 15:15:53,202 - archai.trainers.nlp.nvidia_trainer — INFO —  Epoch: 0 | Step: 1 | Batch: 1 / 1 | LR: 1.000e-05 | ms/batch: 3439.9 | tok/s: 2 | Loss: 10.926 | PPL: 55623.873\n",
      "2023-03-21 15:15:53,210 - archai.trainers.nlp.nvidia_trainer — INFO —  End of training ...\n",
      "2023-03-21 15:15:53,210 - archai.trainers.nlp.nvidia_trainer — INFO —  Training time: 3.462 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from archai.trainers.nlp.nvidia_trainer import NvidiaTrainer\n",
    "from archai.trainers.nlp.nvidia_training_args import NvidiaTrainingArguments\n",
    "\n",
    "# In this example, we will create a dummy dataset with 3 splits\n",
    "data_path = \"dataroot/textpred/olx_tmp/\"\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "with open(data_path + \"train.txt\", \"w\") as f:\n",
    "    f.write(\"train\")\n",
    "with open(data_path + \"valid.txt\", \"w\") as f:\n",
    "    f.write(\"valid\")\n",
    "with open(data_path + \"test.txt\", \"w\") as f:\n",
    "    f.write(\"test\")\n",
    "\n",
    "training_args = NvidiaTrainingArguments(\n",
    "    \"nvidia-gpt2\",\n",
    "    seed=1234,\n",
    "    no_cuda=True,\n",
    "    logging_steps=1,\n",
    "    do_eval=False,\n",
    "    dataset_name=\"olx_tmp\",\n",
    "    dataset_dir=\"./dataroot\",\n",
    "    vocab_type=\"gpt2\",\n",
    "    vocab_size=None,\n",
    "    global_batch_size=1,\n",
    "    seq_len=16,\n",
    "    strategy=\"dp\",\n",
    "    max_steps=1,\n",
    "    optim=\"adam\",\n",
    ")\n",
    "trainer = NvidiaTrainer(model=model, args=training_args)\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "archai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b2be478cf8a2d9a6a1293b022e8589530f7ec0d0340a3a36da6068ef3d344086"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
