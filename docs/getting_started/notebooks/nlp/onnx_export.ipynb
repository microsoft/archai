{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting Models to ONNX\n",
    "\n",
    "Exporting a pre-trained model to ONNX involves converting the model into a common format that can be easily integrated and deployed across different platforms. The conversion can be done using a tool or library, which converts the model's architecture, weights, and configurations. This allows the model to be used in various applications, such as edge devices, cloud services, and web-based systems, with improved compatibility and performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Model\n",
    "\n",
    "The first step is to load any NLP-related model. In this notebook, we will be using a pre-trained GPT-2 model from the Hugging Face's Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting to ONNX\n",
    "\n",
    "After the model has been loaded, we call Archai's `export_to_onnx()` method, which wraps all the inner computation of an ONNX export. Additionally, it supports a set of arguments that can be defined according to the input model and task, such as:\n",
    "\n",
    "* `task`: Task identifier to use proper inputs/outputs.\n",
    "* `use_past`: Whether to include past key/values in the model.\n",
    "* `validate`: Whether to validate the exported model.\n",
    "* `share_weights`: Whether to share the embedding and softmax weights.\n",
    "* `opset`: Set of operations to use with ONNX.\n",
    "* `atol`: Tolerance between input and exported model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-21 15:16:14,303 - archai.onnx.export — INFO —  Exporting model: model.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gderosa\\Anaconda3\\envs\\archai\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:318: TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\n",
      "  past_key, past_value = layer_past\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-21 15:16:28,808 - archai.onnx.export — INFO —  Validating model ...\n",
      "2023-03-21 15:16:28,808 - archai.onnx.onnx_loader — INFO —  Loading model: model.onnx\n",
      "2023-03-21 15:16:30,917 - archai.onnx.export — DEBUG —  Matched outputs: {'present_0', 'present_9', 'present_5', 'present_7', 'present_6', 'present_10', 'present_3', 'present_4', 'present_8', 'present_11', 'present_2', 'present_1', 'probs'}\n",
      "2023-03-21 15:16:30,925 - archai.onnx.export — DEBUG —  Validating output: probs\n",
      "2023-03-21 15:16:30,927 - archai.onnx.export — DEBUG —  Matched shape: (2, 50257) (ONNX) and (2, 50257) (reference)\n",
      "2023-03-21 15:16:30,933 - archai.onnx.export — DEBUG —  Matched difference: 1.0133e-06 < 0.0001\n",
      "2023-03-21 15:16:30,933 - archai.onnx.export — DEBUG —  Validating output: present_0\n",
      "2023-03-21 15:16:30,935 - archai.onnx.export — DEBUG —  Matched shape: (2, 2, 12, 16, 64) (ONNX) and (2, 2, 12, 16, 64) (reference)\n",
      "2023-03-21 15:16:30,935 - archai.onnx.export — DEBUG —  Matched difference: 2.8610e-06 < 0.0001\n",
      "2023-03-21 15:16:30,941 - archai.onnx.export — DEBUG —  Validating output: present_1\n",
      "2023-03-21 15:16:30,942 - archai.onnx.export — DEBUG —  Matched shape: (2, 2, 12, 16, 64) (ONNX) and (2, 2, 12, 16, 64) (reference)\n",
      "2023-03-21 15:16:30,943 - archai.onnx.export — DEBUG —  Matched difference: 4.8876e-06 < 0.0001\n",
      "2023-03-21 15:16:30,944 - archai.onnx.export — DEBUG —  Validating output: present_2\n",
      "2023-03-21 15:16:30,944 - archai.onnx.export — DEBUG —  Matched shape: (2, 2, 12, 16, 64) (ONNX) and (2, 2, 12, 16, 64) (reference)\n",
      "2023-03-21 15:16:30,948 - archai.onnx.export — DEBUG —  Matched difference: 4.2915e-06 < 0.0001\n",
      "2023-03-21 15:16:30,950 - archai.onnx.export — DEBUG —  Validating output: present_3\n",
      "2023-03-21 15:16:30,950 - archai.onnx.export — DEBUG —  Matched shape: (2, 2, 12, 16, 64) (ONNX) and (2, 2, 12, 16, 64) (reference)\n",
      "2023-03-21 15:16:30,954 - archai.onnx.export — DEBUG —  Matched difference: 1.2398e-05 < 0.0001\n",
      "2023-03-21 15:16:30,954 - archai.onnx.export — DEBUG —  Validating output: present_4\n",
      "2023-03-21 15:16:30,958 - archai.onnx.export — DEBUG —  Matched shape: (2, 2, 12, 16, 64) (ONNX) and (2, 2, 12, 16, 64) (reference)\n",
      "2023-03-21 15:16:30,959 - archai.onnx.export — DEBUG —  Matched difference: 1.3351e-05 < 0.0001\n",
      "2023-03-21 15:16:30,959 - archai.onnx.export — DEBUG —  Validating output: present_5\n",
      "2023-03-21 15:16:30,966 - archai.onnx.export — DEBUG —  Matched shape: (2, 2, 12, 16, 64) (ONNX) and (2, 2, 12, 16, 64) (reference)\n",
      "2023-03-21 15:16:30,969 - archai.onnx.export — DEBUG —  Matched difference: 7.6294e-06 < 0.0001\n",
      "2023-03-21 15:16:30,969 - archai.onnx.export — DEBUG —  Validating output: present_6\n",
      "2023-03-21 15:16:30,969 - archai.onnx.export — DEBUG —  Matched shape: (2, 2, 12, 16, 64) (ONNX) and (2, 2, 12, 16, 64) (reference)\n",
      "2023-03-21 15:16:30,975 - archai.onnx.export — DEBUG —  Matched difference: 1.3351e-05 < 0.0001\n",
      "2023-03-21 15:16:30,979 - archai.onnx.export — DEBUG —  Validating output: present_7\n",
      "2023-03-21 15:16:30,979 - archai.onnx.export — DEBUG —  Matched shape: (2, 2, 12, 16, 64) (ONNX) and (2, 2, 12, 16, 64) (reference)\n",
      "2023-03-21 15:16:30,982 - archai.onnx.export — DEBUG —  Matched difference: 7.6294e-06 < 0.0001\n",
      "2023-03-21 15:16:30,982 - archai.onnx.export — DEBUG —  Validating output: present_8\n",
      "2023-03-21 15:16:30,982 - archai.onnx.export — DEBUG —  Matched shape: (2, 2, 12, 16, 64) (ONNX) and (2, 2, 12, 16, 64) (reference)\n",
      "2023-03-21 15:16:30,988 - archai.onnx.export — DEBUG —  Matched difference: 9.0599e-06 < 0.0001\n",
      "2023-03-21 15:16:30,991 - archai.onnx.export — DEBUG —  Validating output: present_9\n",
      "2023-03-21 15:16:30,991 - archai.onnx.export — DEBUG —  Matched shape: (2, 2, 12, 16, 64) (ONNX) and (2, 2, 12, 16, 64) (reference)\n",
      "2023-03-21 15:16:30,996 - archai.onnx.export — DEBUG —  Matched difference: 8.5831e-06 < 0.0001\n",
      "2023-03-21 15:16:30,999 - archai.onnx.export — DEBUG —  Validating output: present_10\n",
      "2023-03-21 15:16:31,001 - archai.onnx.export — DEBUG —  Matched shape: (2, 2, 12, 16, 64) (ONNX) and (2, 2, 12, 16, 64) (reference)\n",
      "2023-03-21 15:16:31,003 - archai.onnx.export — DEBUG —  Matched difference: 7.1526e-06 < 0.0001\n",
      "2023-03-21 15:16:31,005 - archai.onnx.export — DEBUG —  Validating output: present_11\n",
      "2023-03-21 15:16:31,005 - archai.onnx.export — DEBUG —  Matched shape: (2, 2, 12, 16, 64) (ONNX) and (2, 2, 12, 16, 64) (reference)\n",
      "2023-03-21 15:16:31,008 - archai.onnx.export — DEBUG —  Matched difference: 6.9141e-06 < 0.0001\n",
      "Model: 499.167738MB\n"
     ]
    }
   ],
   "source": [
    "from archai.common.file_utils import calculate_onnx_model_size\n",
    "from archai.onnx.export import export_to_onnx\n",
    "\n",
    "onnx_model_path = \"model.onnx\"\n",
    "onnx_config = export_to_onnx(\n",
    "        model,\n",
    "        onnx_model_path,\n",
    "        task=\"causal-lm\",\n",
    "        use_past=True,\n",
    "        share_weights=True,\n",
    "        opset=11,\n",
    "        atol=1e-4,\n",
    "    )\n",
    "print(f\"Model: {calculate_onnx_model_size(onnx_model_path)}MB\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-Export Optimization\n",
    "\n",
    "For Transformer-based models, ONNX Runtime offers a set of post-optimization tools that enables node fusion and hence, a more optimized graph. Thus, we can call `optimize_onnx()` passing the path of the previously exported ONNX model.\n",
    "\n",
    "*The prints compares the models' sizes, but is highly recommended to use an external graph inspection tool, such as Netron.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-21 15:16:32,958 - archai.onnx.optimization — INFO —  Optimizing model: model.onnx\n",
      "Model-OPT: 498.940725MB\n"
     ]
    }
   ],
   "source": [
    "from archai.onnx.optimization import optimize_onnx\n",
    "\n",
    "ort_model_path = optimize_onnx(onnx_model_path, onnx_config, opt_level=1)\n",
    "print(f\"Model-OPT: {calculate_onnx_model_size(ort_model_path)}MB\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-Training Quantization (PTQ)\n",
    "\n",
    "Finally, either the exported or post-optimized models can be dynamically quantized using the `dynamic_quantization_onnx()` method.\n",
    "\n",
    "Nevertheless, please note that if the model has not been pre-trained with Quantization Aware Training (QAT), it might produce different logits and have its performance diminished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-21 15:16:49,535 - archai.quantization.ptq — INFO —  Quantizing model: model-opt.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Failed to infer data type of tensor: /transformer/h.0/attn/Reshape_output_0. Please add data type info for this tensor if your model has customized operators.\n",
      "WARNING:root:Failed to infer data type of tensor: /transformer/h.0/attn/Reshape_1_output_0. Please add data type info for this tensor if your model has customized operators.\n",
      "WARNING:root:Failed to infer data type of tensor: /transformer/h.0/attn/Reshape_2_output_0. Please add data type info for this tensor if your model has customized operators.\n",
      "WARNING:root:Failed to infer data type of tensor: /transformer/h.0/attn/MatMul_1_output_0. Please add data type info for this tensor if your model has customized operators.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignore MatMul due to non constant B: /[/transformer/h.0/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/h.0/attn/MatMul_1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Failed to infer data type of tensor: /transformer/h.1/attn/Reshape_output_0. Please add data type info for this tensor if your model has customized operators.\n",
      "WARNING:root:Failed to infer data type of tensor: /transformer/h.1/attn/Reshape_1_output_0. Please add data type info for this tensor if your model has customized operators.\n",
      "WARNING:root:Failed to infer data type of tensor: /transformer/h.1/attn/Reshape_2_output_0. Please add data type info for this tensor if your model has customized operators.\n",
      "WARNING:root:Failed to infer data type of tensor: /transformer/h.1/attn/MatMul_1_output_0. Please add data type info for this tensor if your model has customized operators.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignore MatMul due to non constant B: /[/transformer/h.1/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/h.1/attn/MatMul_1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Failed to infer data type of tensor: /transformer/h.2/attn/Reshape_output_0. Please add data type info for this tensor if your model has customized operators.\n",
      "WARNING:root:Failed to infer data type of tensor: /transformer/h.2/attn/Reshape_1_output_0. Please add data type info for this tensor if your model has customized operators.\n",
      "WARNING:root:Failed to infer data type of tensor: /transformer/h.2/attn/Reshape_2_output_0. Please add data type info for this tensor if your model has customized operators.\n",
      "WARNING:root:Failed to infer data type of tensor: /transformer/h.2/attn/MatMul_1_output_0. Please add data type info for this tensor if your model has customized operators.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignore MatMul due to non constant B: /[/transformer/h.2/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/h.2/attn/MatMul_1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Failed to infer data type of tensor: /transformer/h.3/attn/Reshape_output_0. Please add data type info for this tensor if your model has customized operators.\n",
      "WARNING:root:Failed to infer data type of tensor: /transformer/h.3/attn/Reshape_1_output_0. Please add data type info for this tensor if your model has customized operators.\n",
      "WARNING:root:Failed to infer data type of tensor: /transformer/h.3/attn/Reshape_2_output_0. Please add data type info for this tensor if your model has customized operators.\n",
      "WARNING:root:Failed to infer data type of tensor: /transformer/h.3/attn/MatMul_1_output_0. Please add data type info for this tensor if your model has customized operators.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignore MatMul due to non constant B: /[/transformer/h.3/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/h.3/attn/MatMul_1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Failed to infer data type of tensor: /transformer/h.4/attn/Reshape_output_0. Please add data type info for this tensor if your model has customized operators.\n",
      "WARNING:root:Failed to infer data type of tensor: /transformer/h.4/attn/Reshape_1_output_0. Please add data type info for this tensor if your model has customized operators.\n",
      "WARNING:root:Failed to infer data type of tensor: /transformer/h.4/attn/Reshape_2_output_0. Please add data type info for this tensor if your model has customized operators.\n",
      "WARNING:root:Failed to infer data type of tensor: /transformer/h.4/attn/MatMul_1_output_0. Please add data type info for this tensor if your model has customized operators.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignore MatMul due to non constant B: /[/transformer/h.4/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/h.4/attn/MatMul_1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Failed to infer data type of tensor: /transformer/h.5/attn/Reshape_output_0. Please add data type info for this tensor if your model has customized operators.\n",
      "WARNING:root:Failed to infer data type of tensor: /transformer/h.5/attn/Reshape_1_output_0. Please add data type info for this tensor if your model has customized operators.\n",
      "WARNING:root:Failed to infer data type of tensor: /transformer/h.5/attn/Reshape_2_output_0. Please add data type info for this tensor if your model has customized operators.\n",
      "WARNING:root:Failed to infer data type of tensor: /transformer/h.5/attn/MatMul_1_output_0. Please add data type info for this tensor if your model has customized operators.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignore MatMul due to non constant B: /[/transformer/h.5/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/h.5/attn/MatMul_1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Failed to infer data type of tensor: /transformer/h.6/attn/Reshape_output_0. Please add data type info for this tensor if your model has customized operators.\n",
      "WARNING:root:Failed to infer data type of tensor: /transformer/h.6/attn/Reshape_1_output_0. Please add data type info for this tensor if your model has customized operators.\n",
      "WARNING:root:Failed to infer data type of tensor: /transformer/h.6/attn/Reshape_2_output_0. Please add data type info for this tensor if your model has customized operators.\n",
      "WARNING:root:Failed to infer data type of tensor: /transformer/h.6/attn/MatMul_1_output_0. Please add data type info for this tensor if your model has customized operators.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignore MatMul due to non constant B: /[/transformer/h.6/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/h.6/attn/MatMul_1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Failed to infer data type of tensor: /transformer/h.7/attn/Reshape_output_0. Please add data type info for this tensor if your model has customized operators.\n",
      "WARNING:root:Failed to infer data type of tensor: /transformer/h.7/attn/Reshape_1_output_0. Please add data type info for this tensor if your model has customized operators.\n",
      "WARNING:root:Failed to infer data type of tensor: /transformer/h.7/attn/Reshape_2_output_0. Please add data type info for this tensor if your model has customized operators.\n",
      "WARNING:root:Failed to infer data type of tensor: /transformer/h.7/attn/MatMul_1_output_0. Please add data type info for this tensor if your model has customized operators.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignore MatMul due to non constant B: /[/transformer/h.7/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/h.7/attn/MatMul_1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Failed to infer data type of tensor: /transformer/h.8/attn/Reshape_output_0. Please add data type info for this tensor if your model has customized operators.\n",
      "WARNING:root:Failed to infer data type of tensor: /transformer/h.8/attn/Reshape_1_output_0. Please add data type info for this tensor if your model has customized operators.\n",
      "WARNING:root:Failed to infer data type of tensor: /transformer/h.8/attn/Reshape_2_output_0. Please add data type info for this tensor if your model has customized operators.\n",
      "WARNING:root:Failed to infer data type of tensor: /transformer/h.8/attn/MatMul_1_output_0. Please add data type info for this tensor if your model has customized operators.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignore MatMul due to non constant B: /[/transformer/h.8/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/h.8/attn/MatMul_1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Failed to infer data type of tensor: /transformer/h.9/attn/Reshape_output_0. Please add data type info for this tensor if your model has customized operators.\n",
      "WARNING:root:Failed to infer data type of tensor: /transformer/h.9/attn/Reshape_1_output_0. Please add data type info for this tensor if your model has customized operators.\n",
      "WARNING:root:Failed to infer data type of tensor: /transformer/h.9/attn/Reshape_2_output_0. Please add data type info for this tensor if your model has customized operators.\n",
      "WARNING:root:Failed to infer data type of tensor: /transformer/h.9/attn/MatMul_1_output_0. Please add data type info for this tensor if your model has customized operators.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignore MatMul due to non constant B: /[/transformer/h.9/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/h.9/attn/MatMul_1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Failed to infer data type of tensor: /transformer/h.10/attn/Reshape_output_0. Please add data type info for this tensor if your model has customized operators.\n",
      "WARNING:root:Failed to infer data type of tensor: /transformer/h.10/attn/Reshape_1_output_0. Please add data type info for this tensor if your model has customized operators.\n",
      "WARNING:root:Failed to infer data type of tensor: /transformer/h.10/attn/Reshape_2_output_0. Please add data type info for this tensor if your model has customized operators.\n",
      "WARNING:root:Failed to infer data type of tensor: /transformer/h.10/attn/MatMul_1_output_0. Please add data type info for this tensor if your model has customized operators.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignore MatMul due to non constant B: /[/transformer/h.10/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/h.10/attn/MatMul_1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Failed to infer data type of tensor: /transformer/h.11/attn/Reshape_output_0. Please add data type info for this tensor if your model has customized operators.\n",
      "WARNING:root:Failed to infer data type of tensor: /transformer/h.11/attn/Reshape_1_output_0. Please add data type info for this tensor if your model has customized operators.\n",
      "WARNING:root:Failed to infer data type of tensor: /transformer/h.11/attn/Reshape_2_output_0. Please add data type info for this tensor if your model has customized operators.\n",
      "WARNING:root:Failed to infer data type of tensor: /transformer/h.11/attn/MatMul_1_output_0. Please add data type info for this tensor if your model has customized operators.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignore MatMul due to non constant B: /[/transformer/h.11/attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/h.11/attn/MatMul_1]\n",
      "Model-QNT: 126.068565MB\n"
     ]
    }
   ],
   "source": [
    "from archai.quantization.ptq import dynamic_quantization_onnx\n",
    "\n",
    "qnt_model_path = dynamic_quantization_onnx(ort_model_path)\n",
    "print(f\"Model-QNT: {calculate_onnx_model_size(qnt_model_path)}MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "archai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b2be478cf8a2d9a6a1293b022e8589530f7ec0d0340a3a36da6068ef3d344086"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
