{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a Custom Dataset Provider\n",
    "\n",
    "Abstract base classes (ABCs) define a blueprint for a class, specifying its methods and attributes, but not its implementation. They are important in implementing a consistent interface, as they enforce a set of requirements on implementing classes and make it easier to write code that can work with multiple implementations.\n",
    "\n",
    "First, we define a boilerplate for the `DatasetProvider` class, which is the same implemented in `archai.api.dataset_provider` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import abstractmethod\n",
    "from typing import Any\n",
    "\n",
    "from overrides import EnforceOverrides\n",
    "\n",
    "\n",
    "class DatasetProvider(EnforceOverrides):\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_train_dataset(self) -> Any:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_val_dataset(self) -> Any:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_test_dataset(self) -> Any:\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torchvision-based Dataset Provider\n",
    "\n",
    "In the context of a custom dataset provider, using ABCs can help ensure that the provider implements the required methods and provides a consistent interface for loading and processing data. In this example, we will implement a Torchvision-based dataset provider, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Optional\n",
    "\n",
    "from overrides import overrides\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.datasets import MNIST, CIFAR10\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "\n",
    "class TorchvisionDatasetProvider(DatasetProvider):\n",
    "    SUPPORTED_DATASETS = {\n",
    "        \"mnist\": MNIST,\n",
    "        \"cifar10\": CIFAR10\n",
    "    }\n",
    "\n",
    "    def __init__(self, dataset: str, root: Optional[str] = \"dataroot\") -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.dataset = dataset\n",
    "        self.root = root\n",
    "\n",
    "    @overrides\n",
    "    def get_train_dataset(\n",
    "        self,\n",
    "        transform: Optional[Callable] = None,\n",
    "        target_transform: Optional[Callable] = None,\n",
    "    ) -> Dataset:\n",
    "        return self.SUPPORTED_DATASETS[self.dataset](\n",
    "            self.root,\n",
    "            train=True,\n",
    "            download=True,\n",
    "            transform=transform or ToTensor(),\n",
    "            target_transform=target_transform,\n",
    "        )\n",
    "\n",
    "    @overrides\n",
    "    def get_val_dataset(\n",
    "        self,\n",
    "        transform: Optional[Callable] = None,\n",
    "        target_transform: Optional[Callable] = None,\n",
    "    ) -> Dataset:\n",
    "        return self.SUPPORTED_DATASETS[self.dataset](\n",
    "            self.root,\n",
    "            train=False,\n",
    "            download=True,\n",
    "            transform=transform or ToTensor(),\n",
    "            target_transform=target_transform,\n",
    "        )\n",
    "\n",
    "    @overrides\n",
    "    def get_test_dataset(\n",
    "        self,\n",
    "        transform: Optional[Callable] = None,\n",
    "        target_transform: Optional[Callable] = None,\n",
    "    ) -> Dataset:\n",
    "        print(f\"Testing set not available for `{self.dataset}`. Returning validation set ...\")\n",
    "        return self.get_val_dataset(transform=transform, target_transform=target_transform)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Dataset Provider\n",
    "\n",
    "Finally, one need to call the implemented methods to retrieve the datasets, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to dataroot\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7757929d89ed42eea274b9f4cbf11a9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataroot\\MNIST\\raw\\train-images-idx3-ubyte.gz to dataroot\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to dataroot\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "320fbc5523d74947b654a15a9dd911df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataroot\\MNIST\\raw\\train-labels-idx1-ubyte.gz to dataroot\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to dataroot\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc44afac91e84f918880360885cb7811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataroot\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to dataroot\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to dataroot\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75249ad3cb9e40ec93da6676c9ca1594",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataroot\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to dataroot\\MNIST\\raw\n",
      "\n",
      "Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: dataroot\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: ToTensor() Dataset MNIST\n",
      "    Number of datapoints: 10000\n",
      "    Root location: dataroot\n",
      "    Split: Test\n",
      "    StandardTransform\n",
      "Transform: ToTensor()\n",
      "Testing set not available for `mnist`. Returning validation set ...\n",
      "Dataset MNIST\n",
      "    Number of datapoints: 10000\n",
      "    Root location: dataroot\n",
      "    Split: Test\n",
      "    StandardTransform\n",
      "Transform: ToTensor()\n"
     ]
    }
   ],
   "source": [
    "dataset_provider = TorchvisionDatasetProvider(\"mnist\")\n",
    "\n",
    "train_dataset = dataset_provider.get_train_dataset()\n",
    "val_dataset = dataset_provider.get_val_dataset()\n",
    "print(train_dataset, val_dataset)\n",
    "\n",
    "# As there is no `test_dataset` available, it returns the validation set\n",
    "test_dataset = dataset_provider.get_test_dataset()\n",
    "print(test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "archai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b2be478cf8a2d9a6a1293b022e8589530f7ec0d0340a3a36da6068ef3d344086"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
