{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a Custom Trainer\n",
    "\n",
    "Abstract base classes (ABCs) define a blueprint for a class, specifying its methods and attributes, but not its implementation. They are important in implementing a consistent interface, as they enforce a set of requirements on implementing classes and make it easier to write code that can work with multiple implementations.\n",
    "\n",
    "First, we define a boilerplate for the `TrainerBase` class, which is the same implemented in `archai.api.trainer_base` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import abstractmethod\n",
    "\n",
    "from overrides import EnforceOverrides\n",
    "\n",
    "\n",
    "class TrainerBase(EnforceOverrides):\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def train(self) -> None:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def evaluate(self) -> None:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self) -> None:\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch-based Trainer\n",
    "\n",
    "In the context of a custom trainer, using ABCs can help ensure that the provider implements the required methods and provides a consistent interface for training, evaluating and predicting. In this example, we will implement a PyTorch-based trainer, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from overrides import overrides\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class PyTorchTrainer(TrainerBase):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: torch.nn.Module,\n",
    "        train_dataset: Optional[Dataset] = None,\n",
    "        eval_dataset: Optional[Dataset] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = model\n",
    "        self.train_dataset = train_dataset\n",
    "        self.eval_dataset = eval_dataset\n",
    "\n",
    "        # Setup the trainer\n",
    "        self._setup()\n",
    "\n",
    "    def _setup(self) -> None:\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "\n",
    "    def _train_step(self, inputs: torch.Tensor, labels: torch.Tensor) -> None:\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        outputs = self.model(inputs)\n",
    "\n",
    "        loss = self.loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    @overrides\n",
    "    def train(self) -> None:\n",
    "        total_loss = 0.0\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(self.train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "        self.model.train()\n",
    "        for idx, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs = inputs.view(inputs.size(0), -1)\n",
    "            \n",
    "            total_loss += self._train_step(inputs, labels)\n",
    "\n",
    "            if idx % 10 == 0:\n",
    "                print(f\"Batch {idx} loss: {total_loss / (idx + 1)}\")\n",
    "\n",
    "    def _eval_step(self, inputs: torch.Tensor, labels: torch.Tensor) -> None:\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.loss_fn(outputs, labels)\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    @overrides\n",
    "    def evaluate(self, eval_dataset: Optional[Dataset] = None) -> None:\n",
    "        eval_dataset = eval_dataset if eval_dataset else self.eval_dataset\n",
    "        assert eval_dataset is not None, \"`eval_dataset` has not been provided.\"\n",
    "\n",
    "        eval_loader = torch.utils.data.DataLoader(eval_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "        eval_loss = 0.0\n",
    "\n",
    "        self.model.eval()\n",
    "        for idx, (inputs, labels) in enumerate(eval_loader):\n",
    "            inputs = inputs.view(inputs.size(0), -1)\n",
    "\n",
    "            loss = self._eval_step(inputs, labels)\n",
    "\n",
    "            eval_loss += loss\n",
    "\n",
    "        self.model.train()\n",
    "\n",
    "        eval_loss /= idx\n",
    "\n",
    "        return eval_loss\n",
    "\n",
    "    @overrides\n",
    "    def predict(self, inputs: torch.Tensor) -> None:\n",
    "        self.model.eval()\n",
    "        preds = self.model(inputs)\n",
    "        self.model.train()\n",
    "\n",
    "        return preds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Model\n",
    "\n",
    "Once the data is loaded, we can define any CV-based model. In this example, we will create a simple linear model using PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(28 * 28, 10)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.linear(x)\n",
    "\n",
    "model = Model()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating and Training with the Trainer\n",
    "\n",
    "After loading the data and creating the data, we need to plug these instances into the `PyTorchTrainer` and start the training, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 loss: 2.3435773849487305\n",
      "Batch 10 loss: 2.164100560274991\n",
      "Batch 20 loss: 2.0146874416442144\n",
      "Batch 30 loss: 1.875573092891324\n",
      "Batch 40 loss: 1.755056075933503\n",
      "Batch 50 loss: 1.65761978486005\n",
      "Batch 60 loss: 1.5680492149024714\n",
      "Batch 70 loss: 1.482287242378987\n",
      "Batch 80 loss: 1.4176807028275948\n",
      "Batch 90 loss: 1.3575652700204115\n",
      "Batch 100 loss: 1.3116845883945427\n",
      "Batch 110 loss: 1.264954976133398\n",
      "Batch 120 loss: 1.2235281644773877\n",
      "Batch 130 loss: 1.1893346013913628\n",
      "Batch 140 loss: 1.1595103922465169\n",
      "Batch 150 loss: 1.1271054373671676\n",
      "Batch 160 loss: 1.098986664173766\n",
      "Batch 170 loss: 1.0724144109159883\n",
      "Batch 180 loss: 1.0449848247496463\n",
      "Batch 190 loss: 1.0206239084610764\n",
      "Batch 200 loss: 1.0005531422237852\n",
      "Batch 210 loss: 0.9785312015863391\n",
      "Batch 220 loss: 0.9595723239814534\n",
      "Batch 230 loss: 0.9406399880394791\n",
      "Batch 240 loss: 0.9242911396926864\n",
      "Batch 250 loss: 0.9074264486947382\n",
      "Batch 260 loss: 0.8933870223746903\n",
      "Batch 270 loss: 0.8793117023482094\n",
      "Batch 280 loss: 0.8656814331685945\n",
      "Batch 290 loss: 0.8519475182511962\n",
      "Batch 300 loss: 0.8420679773207123\n",
      "Batch 310 loss: 0.8304505413368201\n",
      "Batch 320 loss: 0.8197678343343586\n",
      "Batch 330 loss: 0.8108695821999783\n",
      "Batch 340 loss: 0.7994857667303504\n",
      "Batch 350 loss: 0.7905768925308162\n",
      "Batch 360 loss: 0.7818403058270008\n",
      "Batch 370 loss: 0.7724894605717569\n",
      "Batch 380 loss: 0.7644593056262009\n",
      "Batch 390 loss: 0.7569716618494\n",
      "Batch 400 loss: 0.7508149850844148\n",
      "Batch 410 loss: 0.7446284586350703\n",
      "Batch 420 loss: 0.737757377662738\n",
      "Batch 430 loss: 0.731570034354856\n",
      "Batch 440 loss: 0.7251622536623018\n",
      "Batch 450 loss: 0.7184810209234643\n",
      "Batch 460 loss: 0.7108740333685906\n",
      "Batch 470 loss: 0.705083177854048\n",
      "Batch 480 loss: 0.7006571214808743\n",
      "Batch 490 loss: 0.6952635809748818\n",
      "Batch 500 loss: 0.6899459136579327\n",
      "Batch 510 loss: 0.6855154860392942\n",
      "Batch 520 loss: 0.680475171004742\n",
      "Batch 530 loss: 0.6750251736961964\n",
      "Batch 540 loss: 0.6709054712612396\n",
      "Batch 550 loss: 0.6657751858667107\n",
      "Batch 560 loss: 0.6602577523804816\n",
      "Batch 570 loss: 0.6550697249257628\n",
      "Batch 580 loss: 0.6502643423780107\n",
      "Batch 590 loss: 0.6459637832671857\n",
      "Batch 600 loss: 0.6414088696092615\n",
      "Batch 610 loss: 0.6377830508785435\n",
      "Batch 620 loss: 0.6334477859322768\n",
      "Batch 630 loss: 0.6295056212722971\n",
      "Batch 640 loss: 0.625435800731833\n",
      "Batch 650 loss: 0.6220091110046742\n",
      "Batch 660 loss: 0.6180242504501847\n",
      "Batch 670 loss: 0.6146804381900324\n",
      "Batch 680 loss: 0.6113072523680377\n",
      "Batch 690 loss: 0.6073371331098283\n",
      "Batch 700 loss: 0.6044996650344805\n",
      "Batch 710 loss: 0.6016613611356786\n",
      "Batch 720 loss: 0.5985554359574259\n",
      "Batch 730 loss: 0.5963995929618867\n",
      "Batch 740 loss: 0.5928040172970086\n",
      "Batch 750 loss: 0.5907960743227907\n",
      "Batch 760 loss: 0.5879131768011389\n",
      "Batch 770 loss: 0.5846300883322529\n",
      "Batch 780 loss: 0.5818878866150193\n",
      "Batch 790 loss: 0.5786483433055215\n",
      "Batch 800 loss: 0.5762474060467864\n",
      "Batch 810 loss: 0.5737036844364077\n",
      "Batch 820 loss: 0.5711945340527397\n",
      "Batch 830 loss: 0.568284649585702\n",
      "Batch 840 loss: 0.5661225801203112\n",
      "Batch 850 loss: 0.5635929554175546\n",
      "Batch 860 loss: 0.561713579778195\n",
      "Batch 870 loss: 0.5594659192074865\n",
      "Batch 880 loss: 0.5575740954898949\n",
      "Batch 890 loss: 0.555696272622589\n",
      "Batch 900 loss: 0.5535918302197302\n",
      "Batch 910 loss: 0.5511906604179305\n",
      "Batch 920 loss: 0.5482265714001837\n",
      "Batch 930 loss: 0.5461382602454512\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from archai.datasets.cv.mnist_dataset_provider import MnistDatasetProvider\n",
    "\n",
    "dataset_provider = MnistDatasetProvider()\n",
    "train_dataset = dataset_provider.get_train_dataset()\n",
    "\n",
    "trainer = PyTorchTrainer(model, train_dataset=train_dataset)\n",
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating and Predicting with the Trainer\n",
    "\n",
    "Finally, we evaluate our pre-trained model with the validation set and create a set of random-based inputs to calculate the model's predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 0.3360353711610421\n",
      "Predictions: tensor([[-0.1244,  0.2467, -0.0254, -0.0535,  0.0533,  0.1786, -0.0015,  0.1122,\n",
      "         -0.2270, -0.0415]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "val_dataset = dataset_provider.get_val_dataset()\n",
    "\n",
    "eval_loss  = trainer.evaluate(eval_dataset=val_dataset)\n",
    "print(f\"Eval loss: {eval_loss}\")\n",
    "\n",
    "inputs = torch.zeros(1, 28 * 28)\n",
    "preds = trainer.predict(inputs)\n",
    "print(f\"Predictions: {preds}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "archai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b2be478cf8a2d9a6a1293b022e8589530f7ec0d0340a3a36da6068ef3d344086"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
