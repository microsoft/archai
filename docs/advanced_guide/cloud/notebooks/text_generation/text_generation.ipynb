{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Task: Text Generation\n",
        "\n",
        "In this Notebook we run Archai's [Text Generation](https://github.com/microsoft/archai/tree/main/tasks/text_generation) task on Azure Machine Learning.\n",
        "\n",
        "Our goal is to show how to create and run jobs for each step of the task without spending lots of computing resources. Therefore, our goal is not to train a good model -- for this purpose please refer to the original task.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- Python 3.7 or later\n",
        "- An Azure subscription\n",
        "- An Azure Resource Group\n",
        "- An Azure Machine Learning [Workspace](https://learn.microsoft.com/en-us/azure/machine-learning/quickstart-create-resources#create-the-workspace)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install azure-ai-ml azure-identity \n",
        "%pip install jinja2\n",
        "%pip install archai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "from IPython.display import display, Image\n",
        "from IPython.core.display import HTML\n",
        "\n",
        "from azure.ai.ml import load_job\n",
        "\n",
        "import archai.common.azureml_helper as aml_helper\n",
        "import archai.common.notebook_helper as nb_helper"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Get a handle to the workspace\n",
        "\n",
        "We load the workspace from a workspace [configuration file](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-configure-environment#local-and-dsvm-only-create-a-workspace-configuration-file)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ml_client = aml_helper.get_aml_client_from_file(\"../.azureml/config.json\")\n",
        "print(f'Using workspace: {ml_client.workspace_name} in resource group: {ml_client.resource_group_name}')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create a CPU compute cluster\n",
        "\n",
        "We provision a Linux [compute cluster](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-create-attach-compute-cluster?tabs=python) for the NAS job in this Notebook. See the [full list](https://azure.microsoft.com/en-ca/pricing/details/machine-learning/) on VM sizes and prices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cpu_compute_name = \"nas-cpu-cluster-D14-v2\"\n",
        "cpu_compute_cluster = aml_helper.create_compute_cluster(ml_client, cpu_compute_name, size=\"Standard_D14_v2\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create a GPU compute cluster\n",
        "\n",
        "For full training we provision a GPU compute cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gpu_compute_name = \"nas-gpu-cluster-NC6\"\n",
        "gpu_compute_cluster = aml_helper.create_compute_cluster(ml_client, gpu_compute_name, size=\"Standard_NC6\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create an environment based on a YAML file\n",
        "\n",
        "Azure Machine Learning maintains a set of CPU and GPU Ubuntu Linux-based base images with common system dependencies. For the set of base images and their corresponding Dockerfiles, see the [AzureML Containers](https://github.com/Azure/AzureML-Containers) repo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "archai_job_env = aml_helper.create_environment_from_file(ml_client, image=\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:latest\", conda_file=\"conda.yaml\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Job 1: NAS (Searching for Pareto-optimal Architectures)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### Load the search job from a YAML file and run it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675948736346
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "search_job = load_job(source=os.path.join(\"src\", \"search.yaml\"))\n",
        "s_job = ml_client.create_or_update(search_job)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Stream logs of the job\n",
        "\n",
        "This cell will automatically complete when the job is finished."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ml_client.jobs.stream(s_job.name)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Download job's output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output_name = \"output_dir\"\n",
        "download_path = \"output\"\n",
        "\n",
        "aml_helper.download_job_output(ml_client, job_name=s_job.name, output_name=output_name, download_path=download_path)\n",
        "\n",
        "downloaded_folder = Path(download_path) / \"named-outputs\" / output_name"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Show Pareto Frontiers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "param_vs_latency_img = Image(filename=downloaded_folder / \"pareto_non_embedding_params_vs_onnx_latency.png\")\n",
        "display(param_vs_latency_img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "param_vs_memory_img = Image(filename=downloaded_folder / \"pareto_non_embedding_params_vs_onnx_memory.png\")\n",
        "display(param_vs_memory_img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "latency_vs_memory_img = Image(filename=downloaded_folder / \"pareto_onnx_latency_vs_onnx_memory.png\")\n",
        "display(latency_vs_memory_img)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Show search state of the last iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = nb_helper.get_search_csv(downloaded_folder)\n",
        "csv_as_html = nb_helper.get_csv_as_stylized_html(df)\n",
        "display(HTML(csv_as_html))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Job 2: Train (Train a Pareto architecture from Transformer-Flex.)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pick an architecture id (archid) from the CSV file to perform full training on"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "archid = \"<arch-id>\"\n",
        "arch_path = nb_helper.get_arch_abs_path(archid=archid, downloaded_folder=downloaded_folder)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the training job from a YAML file and set the arch_path as its input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_job = load_job(source=os.path.join(\"src\", \"train.yaml\"))\n",
        "train_job.inputs.arch_config_path.path = arch_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "t_job = ml_client.create_or_update(train_job)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Stream logs of the job\n",
        "\n",
        "This cell will automatically complete when the job is finished."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ml_client.jobs.stream(t_job.name)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Job 3: Generating text via prompt"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the generate text job from a YAML file, set the trained model path, and run it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gen_job = load_job(source=os.path.join(\"src\", \"generate_text.yaml\"))\n",
        "gen_job.inputs.pre_trained_model_path.path = \"azureml://full/path/to/trained/model/checkpoint\"\n",
        "\n",
        "g_job = ml_client.create_or_update(gen_job)\n",
        "ml_client.jobs.stream(g_job.name)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Download and show generated text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output_name = \"output_path\"\n",
        "download_path = \"generated_text\"\n",
        "\n",
        "aml_helper.download_job_output(ml_client, job_name=g_job.name, output_name=output_name, download_path=download_path)\n",
        "\n",
        "downloaded_file = Path(download_path) / \"named-outputs\" / output_name / output_name\n",
        "with open(downloaded_file, \"r\") as f:\n",
        "    print(f.read())"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "archai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "0d4293aab49b039cc0cdcd865f65e47bb7838862d84c380ef42d6c53ee313261"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
