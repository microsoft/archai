# Default dataset settings are for cifar
__include__: "./size_224x224_base.yaml"

common:
  seed: 0.0
  # Setup the toy mode used by toy.yaml
  toy_mode:
    max_batches: 25
    train_batch: 64
    test_batch: 64

# Imagenet used only for eval, so search dataset is still cifar10 but eval dataset is imagenet
dataset_eval:
  name: "imagenet"
  n_classes: 1000
  # Number of channels in image
  channels: 3
  # If >= 0 then only these many batches are generated (useful for debugging)
  max_batches: -1
  # Name of folder or tar file to copy from cloud storage
  storage_name: "ImageNet"

nas:
  eval:
    model_desc:
      # Number of cells
      n_cells: 14
      # Stride that aux tower should use
      # 3 is good for 32x32 images and 2 for imagenet
      aux_tower_stride: 2
      dataset:
        _copy: "/dataset_eval"
      model_post_op: "pool_avg2d7x7"
      model_stems:
        ops: ["stem_conv3x3_s4", "stem_conv3x3_s4s2"]
        # Number of input/output channels for nodes in 1st cell
        init_node_ch: 48
        # Output channels multiplier for the stem
        stem_multiplier: 1

    # darts setup
    # loader:
    #   batch: 128
    #   dataset:
    #     _copy: "/dataset_eval"
    # trainer:
    #   apex: # this is overriden in search and eval individually
    #     enabled: False # global switch to disable everything apex
    #     distributed_enabled: False # enable/disable distributed mode
    #   aux_weight: 0.4 # weight for loss from auxiliary towers in test time arch
    #   drop_path_prob: 0.0 # probability that given edge will be dropped
    #   epochs: 250
    #   lossfn: # TODO: this is perhaps reversed for test/train?
    #     type: "CrossEntropyLabelSmooth"
    #     smoothing: 0.1 # label smoothing
    #   optimizer:
    #     lr: 0.1 # init learning rate
    #     decay: 3.0e-5
    #   lr_schedule:
    #     type: "step"
    #     decay_period: 1 # epochs between two learning rate decays
    #     gamma: 0.97 # learning rate decay

    # NVidia benchmark setup DGX1_RN50_AMP_90E.sh
    # Enable amp and distributed 8 GPUs in apex section
    loader:
      batch: 256
      train_workers: 5
      test_workers: 5
      dataset:
        _copy: "/dataset_eval"
    trainer:
      apex:
        # Global switch to disable everything apex-related
        enabled: True
        # Enables/disables distributed mode
        distributed_enabled: True
        # Loss scaling mode for mixed prec, must be string reprenting float or "dynamic"
        loss_scale: "128.0"
      # Weight for loss from auxiliary towers in test time arch
      aux_weight: 0.0
      # Probability that given edge will be dropped
      drop_path_prob: 0.0
      epochs: 250
      # TODO: this is perhaps reversed for test/train?
      lossfn:
        type: "CrossEntropyLabelSmooth"
        # Label smoothing
        smoothing: 0.1
      optimizer:
        # Initial learning rate
        lr: 2.048
        decay: 3.05e-5
        # If .NaN then same as decay otherwise apply different decay to BN layers
        decay_bn: .NaN
        # PyTorch default is 0.0
        momentum: 0.875
      lr_schedule:
        type: "cosine"
        # Minimum learning rate to be set in eta_min param of scheduler
        min_lr: 0.0
        # Increases LR for 0 to current in specified epochs
        # and then hands over to main scheduler
        warmup:
          multiplier: 1.0
          epochs: 8
