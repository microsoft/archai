# Default dataset settings are for cifar
__include__: "../datasets/cifar10.yaml"

common:
  # Should be supplied from command line
  experiment_name: "throwaway"
  experiment_desc: "throwaway"
  logdir: "~/logdir"
  # Prefix for log files that will be created (log.log and log.yaml)
  # No log files will be produced if empty string is used
  log_prefix: "log"
  # Defaults to logging.INFO value
  log_level: 20
  # Should we overwrite existing log file without makng a copy?
  backup_existing_log_file: False
  # If True, structured yaml-based logs are also generated
  yaml_log: True
  seed: 2.0
  # If True, TensorBoard logging is enabled (may impact performance)
  tb_enable: False
  # Path where TensorBoard logs would be stored
  tb_dir: "$expdir/tb"
  checkpoint:
    filename: "$expdir/checkpoint.pth"
    freq: 10

  # Redis address of Ray cluster
  # Use None for single node run, otherwise it should something like host:6379
  # Make sure to run on head node: "ray start --head --redis-port=6379"
  redis: null
  # This is overriden in search and eval individually
  apex:
    # Global switch to disable everything apex-related
    enabled: False
    # Enables/disables distributed mode
    distributed_enabled: True
    # Switch to disable amp mixed precision
    mixed_prec_enabled: True
    # GPU identifiers (comma separated)
    # An empty string will use all GPUs
    gpus: ""
    # Optimization level for mixed precision
    opt_level: "O2"
    # Keep BN in 32-floating point (fp32) precision
    bn_fp32: True
    # Loss scaling mode for mixed prec, must be string reprenting float or "dynamic"
    loss_scale: "dynamic"
    # Whether BNs should be replaced with sync BNs for distributed model
    sync_bn: False
    # Enable/disable distributed mode
    scale_lr: True
    # Allows to confirm we are using a distributed setting
    min_world_size: 0
    # If True, PyTorch code will run 6x slower
    detect_anomaly: False
    seed: "_copy: /common/seed"
    ray:
      # Initialize ray, but note that ray cannot be used if apex distributed is enabled
      enabled: False
      # If True ray will run in serial mode
      local_mode: False

  smoke_test: False
  only_eval: False
  resume: True

# Default dataset settings comes from __include__ on the top
dataset: {}

nas:
  eval:
    # Model description used for building model for evaluation
    full_desc_filename: "$expdir/full_model_desc.yaml"
    # Model description used as template to construct cells
    final_desc_filename: "$expdir/final_model_desc.yaml"

    # If below is specified then final_desc_filename is ignored and
    # model is created through factory function instead;
    # This is useful for running eval for manually designed models such as resnet-50;
    # The value is string of form "some.namespace.module.function",
    # where the function returns nn.Module and no required args
    model_factory_spec: ""

    metric_filename: "$expdir/eval_train_metrics.yaml"
    # File to where trained model will be saved
    model_filename: "$expdir/model.pt"
    data_parallel: False
    checkpoint:
      _copy: "/common/checkpoint"
    resume: "_copy: /common/resume"
    model_desc:
      # Number of reductions to be applied
      n_reductions: 2
      # Number of cells
      n_cells: 20
      dataset:
        _copy: "/dataset"
      # Maximum edges that can be in final arch per node
      max_final_edges: 2
      model_post_op: "pool_adaptive_avg2d"
      # Additional custom parameters for model description
      params: {}
      # Weight for loss from auxiliary towers in test time arch
      aux_weight: 0.4
      # Stride that aux tower should use
      # 3 is good for 32x32 images and 2 for imagenet
      aux_tower_stride: 3
      model_stems:
        ops: ["stem_conv3x3", "stem_conv3x3"]
        # Number of input/output channels for nodes in 1st cell
        init_node_ch: 36
        # Output channels multiplier for the stem
        stem_multiplier: 3
      cell:
        # Number of nodes in a cell
        n_nodes: 4
        cell_post_op: "concate_channels"
    loader:
      apex:
        _copy: "../../trainer/apex"
      # Additional augmentations to use
      # Example: fa_reduced_cifar10, arsaug, autoaug_cifar10 and autoaug_extend
      aug: ""
      # Cutout length
      # Use cutout augmentation when > 0
      cutout: 16
      # Load train split of dataset
      load_train: True
      # 96 is too aggressive for 1080Ti, better set it to 68
      train_batch: 96
      train_workers: 4
      # If null, will be used 4
      test_workers: "_copy: ../train_workers"
      # Load test split of dataset
      load_test: True
      test_batch: 1024
      # Split portion for test set, 0 to 1
      val_ratio: 0.0
      # Fold number to use (0 to 4)
      val_fold: 0
      # Total number of cross-validation folds available
      cv_num: 5
      dataset:
        _copy: "/dataset"
    trainer:
      apex:
        _copy: "/common/apex"
      aux_weight: "_copy: /nas/eval/model_desc/aux_weight"
      # Probability that given edge will be dropped
      drop_path_prob: 0.2
      # Gradients above this value is clipped
      grad_clip: 5.0
      # wWight to be applied to sum(abs(alphas)) to loss term
      l1_alphas: 0.0
      # After every N updates dump loss and other metrics in logger
      logger_freq: 1000
      title: "eval_train"
      epochs: 600
      # Split batch into these many chunks and accumulate gradients,
      # so we can support GPUs with lower RAM
      batch_chunks: 1
      lossfn:
        type: "CrossEntropyLoss"
      optimizer:
        type: "sgd"
        # Initial learning rate
        lr: 0.025
        # PyTorch default is 0.0
        decay: 3.0e-4
        # PyTorch default is 0.0
        momentum: 0.9
        # PyTorch default is False
        nesterov: False
        # If .NaN then same as decay, otherwise apply different decay to BN layers
        decay_bn: .NaN
      lr_schedule:
        type: "cosine"
        # Minimum learning rate to be set in eta_min param of scheduler
        min_lr: 0.001
        # Increases LR for 0 to current in specified epochs and then hands over to main scheduler
        warmup:
          multiplier: 1
          # 0 to disable warmup
          epochs: 0
      validation:
        title: "eval_test"
        # Split batch into these many chunks and accumulate gradients,
        # so we can support GPUs with lower RAM
        batch_chunks: "_copy: ../../batch_chunks"
        logger_freq: 0
        # Performs validation only every N epochs
        freq: 1
        lossfn:
          type: "CrossEntropyLoss"

  search:
    # Options are "random" or "default"
    finalizer: "default"
    data_parallel: False
    checkpoint:
      _copy: "/common/checkpoint"
    resume: "_copy: /common/resume"
    search_iters: 1
    # File to save the arch before it was finalized
    full_desc_filename: "$expdir/full_model_desc.yaml"
    # File to save the final arch
    final_desc_filename: "$expdir/final_model_desc.yaml"
    # Where metrics and model stats would be saved for each pareto iteration
    metrics_dir: "$expdir/models/{reductions}/{cells}/{nodes}/{search_iter}"
    seed_train:
      trainer:
        _copy: "/nas/eval/trainer"
        title: "seed_train"
        # Number of epochs that model will be trained before search
        epochs: 0
        aux_weight: 0.0
        drop_path_prob: 0.0
      loader:
        _copy: "/nas/eval/loader"
        train_batch: 128
        # Split portion for test set, 0 to 1
        val_ratio: 0.1
    post_train:
      trainer:
        _copy: "/nas/eval/trainer"
        title: "post_train"
        # Number of epochs that model will be trained after search
        epochs: 0
        aux_weight: 0.0
        drop_path_prob: 0.0
      loader:
        _copy: "/nas/eval/loader"
        train_batch: 128
        # Split portion for test set, 0 to 1
        val_ratio: 0.1
    pareto:
      # Default parameters are set so there is exactly one search iteration
      max_cells: 8
      max_reductions: 2
      max_nodes: 4
      enabled: False
      # For each iteration of macro, we save model and performNCE summary
      summary_filename: "$expdir/pareto.tsv"
    model_desc:
      # Number of reductions to be applied
      n_reductions: 2
      # Number of cells
      n_cells: 8
      # Aavoid copying from eval node because dataset settings
      # may override eval.model_desc with different stems, pool, etc
      dataset:
        _copy: "/dataset"
      # Maximum edge that can be in final arch be node
      max_final_edges: 2
      model_post_op: "pool_adaptive_avg2d"
      params: {}
      # Weight for loss from auxiliary towers in test time arch
      aux_weight: 0.0
      # Stride that aux tower should use
      # 3 is good for 32x32 images and 2 for imagenet
      aux_tower_stride: 3
      model_stems:
        ops: ["stem_conv3x3", "stem_conv3x3"]
        # Output channels multiplier for the stem
        stem_multiplier: 3
        # Number of input/output channels for nodes in 1st cell
        init_node_ch: 16
      cell:
        # Number of nodes in a cell
        n_nodes: 4
        cell_post_op: "concate_channels"
    loader:
      apex:
        _copy: "../../trainer/apex"
      # Additional augmentations to use
      # Example: fa_reduced_cifar10, arsaug, autoaug_cifar10 and autoaug_extend
      aug: ""
      # Cutout length
      # Use cutout augmentation when > 0
      cutout: 0
      # Load train split of dataset
      load_train: True
      train_batch: 64
      # If null will default to gpu_count * 4
      train_workers: 4
      # If null will default to 4
      test_workers: "_copy: ../train_workers"
      # Load test split of dataset
      load_test: False
      test_batch: 1024
      # Split portion for test set, 0 to 1
      val_ratio: 0.5
      # Folder number to use (0 to 4)
      val_fold: 0
      # Total number of cross-validation folds available
      cv_num: 5
      dataset:
        _copy: "/dataset"
    trainer:
      apex:
        _copy: "/common/apex"
      aux_weight: "_copy: /nas/search/model_desc/aux_weight"
      # Probability that given edge will be dropped
      drop_path_prob: 0.0
      # Gradients above this value are clipped
      grad_clip: 5.0
      # After every N updates, dump loss and other metrics in logger
      logger_freq: 1000
      title: "arch_train"
      epochs: 50
      # Split batch into these many chunks and accumulate gradients,
      # so we can support GPUs with lower RAM
      batch_chunks: 1
      # Empty string means no plots, otherwise plots are generated for each epoch in this dir
      plotsdir: ""
      # Weight to be applied to sum(abs(alphas)) to loss term
      l1_alphas: 0.0
      lossfn:
        type: "CrossEntropyLoss"
      optimizer:
        type: "sgd"
        # Initial learning rate
        lr: 0.025
        decay: 3.0e-4
        # PyTorch default is 0.0
        momentum: 0.9
        nesterov: False
        # If .NaN then same as decay, otherwise apply different decay to BN layers
        decay_bn: .NaN
      alpha_optimizer:
        type: "adam"
        lr: 3.0e-4
        decay: 1.0e-3
        betas: [0.5, 0.999]
        # If .NaN then same as decay, otherwise apply different decay to BN layers
        decay_bn: .NaN
      alpha_lr_schedule:
        type: ""
      lr_schedule:
        type: "cosine"
        # Minimum learning rate to be set in eta_min param of scheduler
        min_lr: 0.001
        warmup: null
      validation:
        title: "search_val"
        logger_freq: 0
        # Split batch into these many chunks and accumulate gradients,
        # so we can support GPUs with lower RAM
        batch_chunks: "_copy: ../../batch_chunks"
        # Perform validation only every N epochs
        freq: 1
        lossfn:
          type: "CrossEntropyLoss"

autoaug:
  num_op: 2
  num_policy: 5
  num_search: 200
  # After conducting N trials, choose the results of top num_result_per_cv
  num_result_per_cv: 10
  loader:
    apex:
      _copy: "/common/apex"
    # Additional augmentations to use
    # Example: fa_reduced_cifar10, arsaug, autoaug_cifar10 and autoaug_extend
    aug: ""
    # Cutout length
    # Use cutout augmentation when > 0
    cutout: 16
    epochs: 50
    # Load train split of dataset
    load_train: True # load train split of dataset
    train_batch: 64
    # If null will default to gpu_count * 4
    train_workers: 4
    # If null will default to 4
    test_workers: "_copy: ../train_workers"
    # Load test split of dataset
    load_test: True
    test_batch: 1024
    # Split portion for test set, 0 to 1
    val_ratio: 0.4
    # Folder number to use (0 to 4)
    val_fold: 0
    # Total number of cross-validation folds available
    cv_num: 5
    dataset:
      _copy: "/dataset"
  optimizer:
    type: "sgd"
    # Initial learning rate
    lr: 0.025
    # PyTorch default is 0.0
    decay: 3.0e-4
    # PyTorch default is 0.0
    momentum: 0.9 # pytorch default is 0.0
    # PyTorch default is False
    nesterov: False
    # Gradients above this value are clipped
    # TODO: Why is this also in trainer?
    clip: 5.0
    # If .NaN then same as decay, otherwise apply different decay to BN layers
    decay_bn: .NaN
    # PyTorch defualt betas for Adam optimizer
    # betas: [0.9, 0.999]
  lr_schedule:
    type: "cosine"
    # Minimum learning rate to be set in eta_min param of scheduler
    min_lr: 0.0
    warmup: null
