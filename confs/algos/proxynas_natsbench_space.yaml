__include__: 'darts.yaml' # just use darts defaults


nas:
  search:
    model_desc:
      num_edges_to_sample: 2 # number of edges each node will take input from

  eval:
    natsbench:
      arch_index: 6758
      natsbench_tss_fast: 'NATS-tss-v1_0-3ffb9-simple' # folder name in dataroot/natsbench that contains the tss fast mode folder
    model_desc:
      num_edges_to_sample: 2
    loader:
      train_batch: 256
      aug: '' # in natsbench paper they use random flip and crop, which are part of the regular transforms
      naswotrain:
        train_batch: 256 # batch size for computing trainingless score
      freeze_loader:
        train_batch: 512 # batch size for freeze training. 2048 works reliably on V100 with cell13 onwards unfrozen
    trainer:
      plotsdir: ''
      train_top1_acc_threshold: 0.6 # after some accuracy we will shift into training only the last 'n' layers
      apex:
        _copy: '/common/apex'
      aux_weight: '_copy: /nas/eval/model_desc/aux_weight'
      drop_path_prob: 0.2 # probability that given edge will be dropped
      grad_clip: 5.0 # grads above this value is clipped
      l1_alphas: 0.0   # weight to be applied to sum(abs(alphas)) to loss term
      logger_freq: 1000 # after every N updates dump loss and other metrics in logger
      title: 'eval_train'
      epochs: 200
      batch_chunks: 1 # split batch into these many chunks and accumulate gradients so we can support GPUs with lower RAM
      lossfn:
        type: 'CrossEntropyLoss'
      optimizer:
        type: 'sgd'
        lr: 0.1 # init learning rate
        decay: 5.0e-4 # pytorch default is 0.0
        momentum: 0.9 # pytorch default is 0.0
        nesterov: True # pytorch default is False
        decay_bn: .NaN # if NaN then same as decay otherwise apply different decay to BN layers
      lr_schedule:
        type: 'cosine'
        min_lr: 0.000 # min learning rate to be set in eta_min param of scheduler
        warmup:  # increases LR for 0 to current in specified epochs and then hands over to main scheduler
          multiplier: 1
          epochs: 0 # 0 disables warmup
  
    freeze_trainer:
      plotsdir: ''
      identifiers_to_unfreeze: ['classifier', 'lastact', 'cells.16'] # last few layer names in natsbench: lastact, lastact.0, lastact.1: BN-Relu, global_pooling: global avg. pooling (doesn't get exposed as a named param though), classifier: linear layer
      apex:
        _copy: '/common/apex'
      aux_weight: 0.0 # very important that this is 0.0 for freeze training
      drop_path_prob: 0.0 # very important that this is 0.0 for freeze training
      grad_clip: 5.0 # grads above this value is clipped
      l1_alphas: 0.0  # weight to be applied to sum(abs(alphas)) to loss term
      logger_freq: 1000 # after every N updates dump loss and other metrics in logger
      title: 'eval_train'
      epochs: 10
      batch_chunks: 1 # split batch into these many chunks and accumulate gradients so we can support GPUs with lower RAM
      lossfn:
        type: 'CrossEntropyLoss'
      optimizer:
        type: 'sgd'
        lr: 0.1 # init learning rate
        decay: 5.0e-4 # pytorch default is 0.0
        momentum: 0.9 # pytorch default is 0.0
        nesterov: True # pytorch default is False
        decay_bn: .NaN # if NaN then same as decay otherwise apply different decay to BN layers
      lr_schedule:
        type: 'cosine'
        min_lr: 0.000 # min learning rate to be set in eta_min param of scheduler
        warmup:  # increases LR for 0 to current in specified epochs and then hands over to main scheduler
          multiplier: 1
          epochs: 0 # 0 disables warmup
      validation:
        title: 'eval_test'
        batch_chunks: '_copy: ../../batch_chunks' # split batch into these many chunks and accumulate gradients so we can support GPUs with lower RAM
        logger_freq: 0
        freq: 1 # perform validation only every N epochs
        lossfn:
          type: 'CrossEntropyLoss'