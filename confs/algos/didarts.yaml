# Use darts defaults
__include__: "darts.yaml"

nas:
  search:
    trainer:
      epochs: 200
      alpha_optimizer:
        type: "sgd"
        # Initial learning rate
        lr: 0.025
        decay: 3.0e-4
        # PyTorch default is 0.0
        momentum: 0.9
        nesterov: False
        # If .NaN then same as decay, otherwise apply different decay to BN layers
        decay_bn: .NaN
      alpha_lr_schedule:
        type: "cosine"
        # Minimum learning rate to be set in eta_min param of scheduler
        # Alternative value: 0.001
        min_lr: 0.0
        warmup: null
    loader:
      # Split portion for test set, 0 to 1
      val_ratio: 0.1
