__include__: "darts.yaml" # just use darts defaults

nas:
  search:
    trainer:
      epochs: 200
      alpha_optimizer:
        type: 'sgd'
        lr: 0.025 # init learning rate
        decay: 3.0e-4
        momentum: 0.9 # pytorch default is 0
        nesterov: False
        decay_bn: .NaN # if NaN then same as decay otherwise apply different decay to BN layers
      alpha_lr_schedule:
        type: 'cosine'
        min_lr: 0.0 # 0.001 min learning rate, this will be used in eta_min param of scheduler
        warmup: null
    loader:
      val_ratio: 0.1 #split portion for test set, 0 to 1