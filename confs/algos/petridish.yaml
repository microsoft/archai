__include__: 'darts.yaml' # defaults are loaded from this file

common:
  #yaml_log: False
  apex:
    enabled: False # global switch to disable everything apex
    distributed_enabled: False # enable/disable distributed mode
    ray:
      enabled: True # initialize ray. Note: ray cannot be used if apex distributed is enabled
      local_mode: False # if True then ray runs in serial mode

nas:
  eval:
    final_desc_foldername: '$expdir/model_desc_gallery' #
    source_desc_foldername: '$expdir/model_desc_gallery'
    model_desc:
      n_reductions: 2 # number of reductions to be applied
      n_cells: 10 # number of max cells, for pareto frontier, we use cell_count_scale to multiply cells and limit by n_cells
      aux_weight: 0.0 # weight for loss from auxiliary towers in test time arch
      num_edges_to_sample: 2 # number of edges each node will take inputs from
      aux_tower_stride: 3
      model_stems:
        ops: ['stem_conv3x3_s2', 'stem_conv3x3_s2']
        init_node_ch: 32 # num of input/output channels for nodes in 1st cell
        stem_multiplier: 1 # output channels multiplier for the stem
      cell:
        n_nodes: 5 # number of nodes in a cell if template desc is not provided
        cell_post_op: 'proj_channels'
    petridish:
      cell_count_scale: 1.0 # for eval first multiply number of cells used in search by this factor, limit to n_cells
    trainer:
      aux_weight: 0.0
      epochs: 1500
      batch_chunks: 1
      validation:
        batch_chunks: 1
      optimizer:
          lr: 0.033
    loader:
      cutout: 6 # cutout length, use cutout augmentation when > 0
      load_train: True # load train split of dataset
      train_batch: 32 
      test_batch: 32
      img_size: 16
      aug: 'autoaug_cifar10'
      # dataset:
      #  max_batches: 32
      
  search:
    final_desc_foldername: '$expdir/model_desc_gallery' # the gallery of models that eval will train from scratch
    petridish:
      convex_hull_eps: 0.025 # tolerance
      max_madd: 20000000 # if any parent model reaches this many multiply-additions then the search is terminated or it reaches maximum number of parent pool size
      max_hull_points: 100 # if the pool of parent models reaches this size then search is terminated or if it reaches max multiply-adds
      checkpoints_foldername: '$expdir/petridish_search_checkpoints'
    pareto:
      max_cells: 10
      max_reductions: 2
      max_nodes: 3
      enabled: True # if false then there will only be one seed model. if true a number of seed models with different number of cells, reductions and nodes will be used to initialize the search. this provides more coverage of the frontier.
    model_desc:
      n_cells: 3
      n_reductions: 1
      num_edges_to_sample: 2 # number of edges each node will take inputs from
      cell:
        n_nodes: 1 # also used as min nodes to get combinations for seeding pareto
        cell_post_op: 'proj_channels'
      model_stems:
        ops: ['stem_conv3x3_s2', 'stem_conv3x3_s2']
        stem_multiplier: 1 # output channels multiplier for the stem
        init_node_ch: 32 # num of input/output channels for nodes in 1st cell
    seed_train:
      trainer:
        epochs: 80 # number of epochs model will be trained before search
        optimizer:
          lr: 0.033
        batch_chunks: 1
        validation:
          batch_chunks: 1
      loader:
        cutout: 6
        train_batch: 32
        test_batch: 32
        img_size: 16
        aug: ''
        # dataset:
        #  max_batches: 32
    post_train:
      trainer:
        epochs: 80 # number of epochs model will be trained after search
        optimizer:
          lr: 0.033
      loader:
        train_batch: 32
        cutout: 6
        test_batch: 32
        img_size: 16
        aug: ''
        # dataset:
        #   max_batches: 32
    trainer:
      l1_alphas:  0.001   # as per paper
      epochs: 80 # number of epochs model will be trained during search
      optimizer:
          lr: 0.033
    loader:
      train_batch: 32
      val_ratio: 0.2 #split portion for train set, 0 to 1
      cutout: 6
      test_batch: 32
      img_size: 16
      aug: ''
      # dataset:
      #   max_batches: 32