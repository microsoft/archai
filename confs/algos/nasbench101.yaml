# Use darts defaults
__include__: "darts.yaml"

common:
  # Defaults to logging.INFO value
  log_level: 20

nas:
  eval:
    model_desc:
      params: {
          "cell_matrix":
            [
              [0, 1, 1, 1, 0, 1, 0],
              [0, 0, 0, 0, 0, 0, 1],
              [0, 0, 0, 0, 0, 0, 1],
              [0, 0, 0, 0, 1, 0, 0],
              [0, 0, 0, 0, 0, 0, 1],
              [0, 0, 0, 0, 0, 0, 1],
              [0, 0, 0, 0, 0, 0, 0],
            ],
          "vertex_ops":
            [
              "input",
              "conv1x1-bn-relu",
              "conv3x3-bn-relu",
              "conv3x3-bn-relu",
              "conv3x3-bn-relu",
              "maxpool3x3",
              "output",
            ],
          # Number of stacks, where each contains identical cells
          "num_stacks": 3,
        }
      model_stems:
        ops: ["stem_conv3x3Relu", "stem_conv3x3Relu"]
        # Output channels for stem = 128
        stem_multiplier: 1
        # Number of input/output channels for nodes in 1st cell
        init_node_ch: 128
      model_post_op: "pool_mean_tensor"
      # 3 stacks, where each stack has 3 cells
      n_cells: 9
    loader:
      # Additional augmentations to use
      # Example: fa_reduced_cifar10, arsaug, autoaug_cifar10 and autoaug_extend
      aug: ""
      # Cutout length
      # Use cutout augmentation when > 0
      cutout: 0
      # 96 is too aggressive for 1080Ti, better set it to 68
      train_batch: 128
    trainer:
      aux_weight: 0.0
      # Probability that given edge will be dropped
      drop_path_prob: 0.0
      # Gradients above this value is clipped
      grad_clip: 5.0
      epochs: 108
      optimizer:
        type: "sgd"
        # Initial learning rate
        lr: 0.025
        # PyTorch default is 0.0
        decay: 1.0e-4
        # PyTorch default is 0.0
        momentum: 0.9
        # PyTorch default is False
        nesterov: False
      lr_schedule:
        type: "cosine"
        # Minimum learning rate to be set in eta_min param of scheduler
        min_lr: 0.0
