description: Train Transformer XL

target:
  service: amlk8s
  name: ms-shared-v100 # ms-shared-v100, itpscusv100cl  itpseasiav100cl  itplabrr1cl1  itpscusv100cl itpwus2v100cl
  vc: resrchvc

environment:
  image: pytorch/pytorch:1.6.0-cuda10.1-cudnn7-devel
  setup:
    - set -e -o xtrace
    - sudo apt-get -y install git
    - pip install --user tensorboard

storage:
  us_east:
    storage_account_name: 'archaieast'
    container_name: 'phillytools'

code:
  local_dir: /home/t-gjawahar/archai

data:
  storage_id: 'us_east'
  remote_dir: dataroot

search:
  job_template:
    name: 'train_xxl_vocab{vocab_size}_{config}'
    sku: G8
    command:
      - set -e -o xtrace
      - bash scripts/apex_install.sh
      - pip install --user -e .
      - python -m torch.distributed.launch --nproc_per_node="1" archai/nlp/nvidia_transformer_xl/train.py --config {config} --config_file wt103_base.yaml --vocab_size {vocab_size}  --n_layer 16 --n_head 8 --d_model 256 --d_head 32 --d_inner 768 # --lr 0.0005 --max_step 200000 --batch_size 512
  type: grid
  max_trials: 1

  params:
    - name: config
      spec: discrete
      values: ['dgx1_1gpu_fp16'] # dgx1_8gpu_fp16, dgx1_1gpu_fp16, toy, default, dgx1_4gpu_fp16
    - name: vocab_size
      spec: discrete
      values: [100]
