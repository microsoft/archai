
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Quantization &#8212; Archai</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/custom.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/tabs.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
    <script>mermaid.initialize({startOnLoad:true});</script>
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Natural Language Processing" href="archai.quantization.nlp.html" />
    <link rel="prev" title="Optimization Utilities" href="archai.onnx.optimization_utils.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/logo.png" class="logo" alt="logo">
      
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Getting Started
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../getting_started/installation.html">
   Installation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../getting_started/package_structure.html">
   Package Structure
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../getting_started/quick_start.html">
   Quick Start
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../getting_started/notebooks.html">
   Notebooks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../getting_started/notebooks/api.html">
     API
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
    <label for="toctree-checkbox-2">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../getting_started/notebooks/api/dataset_provider.html">
       Dataset Provider
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../getting_started/notebooks/api/trainer_base.html">
       Trainer (Base)
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../getting_started/notebooks/discrete_search.html">
     Discrete Search
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
    <label for="toctree-checkbox-3">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../getting_started/notebooks/discrete_search/search_space.html">
       Search Spaces
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../getting_started/notebooks/discrete_search/evaluators.html">
       Evaluators
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../getting_started/notebooks/discrete_search/algos.html">
       Algorithms
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../getting_started/notebooks/discrete_search/config_search.html">
       Configuration-based Search
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../getting_started/notebooks/supergraph.html">
     Supergraph
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../getting_started/notebooks/cv.html">
     Computer Vision
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
    <label for="toctree-checkbox-4">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../getting_started/notebooks/cv/cv_dataset_provider.html">
       Dataset Provider
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../getting_started/notebooks/cv/pl_trainer.html">
       PyTorch-Lightining Trainer
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../getting_started/notebooks/nlp.html">
     Natural Language Processing
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
    <label for="toctree-checkbox-5">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../getting_started/notebooks/nlp/hf_dataset_provider.html">
       HF Dataset Provider
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../getting_started/notebooks/nlp/hf_trainer.html">
       HF Trainer
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../getting_started/notebooks/nlp/nvidia_dataset_provider.html">
       NVIDIA Dataset Provider
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../getting_started/notebooks/nlp/nvidia_trainer.html">
       NVIDIA Trainer
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../getting_started/notebooks/nlp/onnx_export.html">
       ONNX Export
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../getting_started/notebooks/nlp/torch_quantization.html">
       PyTorch Quantization
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Advanced Guide
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../advanced_guide/nas.html">
   Neural Architecture Search
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../advanced_guide/nas/discrete_search.html">
     Discrete Search
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../advanced_guide/nas/supergraph.html">
     Supergraph
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../advanced_guide/cv.html">
   Computer Vision
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../advanced_guide/cv/datasets.html">
     Datasets
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../advanced_guide/cv/trainers.html">
     Trainers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../advanced_guide/cv/onnx.html">
     ONNX
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../advanced_guide/cv/quantization.html">
     Quantization
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../advanced_guide/nlp.html">
   Natural Language Processing
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../advanced_guide/nlp/datasets.html">
     Datasets
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../advanced_guide/nlp/trainers.html">
     Trainers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../advanced_guide/nlp/onnx.html">
     ONNX
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../advanced_guide/nlp/quantization.html">
     Quantization
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../advanced_guide/cloud.html">
   Cloud-Based Search
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../advanced_guide/cloud/azure.html">
     Azure
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Contributing
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../contributing/first_contribution.html">
   First Time Contributor
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../contributing/documentation.html">
   Documentation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../contributing/unitary_tests.html">
   Unitary Tests
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Support
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../support/faq.html">
   Frequently Asked Questions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../support/contact.html">
   Contact
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../support/copyright.html">
   Copyright
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Reference
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="../api.html">
   API
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="archai.api.html">
     API
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="archai.common.html">
     Common Packages
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="archai.datasets.html">
     Datasets
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
    <label for="toctree-checkbox-11">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="archai.datasets.cv.html">
       Computer Vision
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
      <label for="toctree-checkbox-12">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4">
        <a class="reference internal" href="archai.datasets.cv.transforms.html">
         Transforms
        </a>
       </li>
      </ul>
     </li>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="archai.datasets.nlp.html">
       Natural Language Processing
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
      <label for="toctree-checkbox-13">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4">
        <a class="reference internal" href="archai.datasets.nlp.tokenizer_utils.html">
         Tokenization Utilities
        </a>
       </li>
      </ul>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="archai.discrete_search.html">
     Discrete Search
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/>
    <label for="toctree-checkbox-14">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="archai.discrete_search.algos.html">
       Search Algorithms
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="archai.discrete_search.api.html">
       API
      </a>
     </li>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="archai.discrete_search.evaluators.html">
       Evaluators
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/>
      <label for="toctree-checkbox-15">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4">
        <a class="reference internal" href="archai.discrete_search.evaluators.nlp.html">
         Natural Language Processing
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="archai.discrete_search.evaluators.pt_profiler_utils.html">
         PyTorch Profiler (Utilities)
        </a>
       </li>
      </ul>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="archai.discrete_search.predictors.html">
       Predictors
      </a>
     </li>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="archai.discrete_search.search_spaces.html">
       Search Spaces
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/>
      <label for="toctree-checkbox-16">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4">
        <a class="reference internal" href="archai.discrete_search.search_spaces.config.html">
         Configuration-Based
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="archai.discrete_search.search_spaces.natsbench_tss.html">
         NATS-Bench
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="archai.discrete_search.search_spaces.nlp.html">
         Natural Language Processing
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="archai.discrete_search.search_spaces.segmentation_dag.html">
         Segmentation DAG
        </a>
       </li>
      </ul>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="archai.discrete_search.utils.html">
       Utilities
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="archai.onnx.html">
     ONNX
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/>
    <label for="toctree-checkbox-17">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="archai.onnx.config_utils.html">
       Configuration Utilities
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="archai.onnx.optimization_utils.html">
       Optimization Utilities
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 current active has-children">
    <a class="current reference internal" href="#">
     Quantization
    </a>
    <input checked="" class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" type="checkbox"/>
    <label for="toctree-checkbox-18">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="archai.quantization.nlp.html">
       Natural Language Processing
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="archai.supergraph.html">
     Supergraph
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" type="checkbox"/>
    <label for="toctree-checkbox-19">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="archai.supergraph.algos.html">
       Algorithms
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" type="checkbox"/>
      <label for="toctree-checkbox-20">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4">
        <a class="reference internal" href="archai.supergraph.algos.darts.html">
         DARTS
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="archai.supergraph.algos.didarts.html">
         DiDARTS
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="archai.supergraph.algos.divnas.html">
         DivNAS
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="archai.supergraph.algos.gumbelsoftmax.html">
         Gumbel-Softmax
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="archai.supergraph.algos.manual.html">
         Manual
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="archai.supergraph.algos.nasbench101.html">
         NasBench-101
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="archai.supergraph.algos.petridish.html">
         Petridish
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="archai.supergraph.algos.random.html">
         Random
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="archai.supergraph.algos.xnas.html">
         XNAS
        </a>
       </li>
      </ul>
     </li>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="archai.supergraph.datasets.html">
       Datasets
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-21" name="toctree-checkbox-21" type="checkbox"/>
      <label for="toctree-checkbox-21">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4">
        <a class="reference internal" href="archai.supergraph.datasets.providers.html">
         Providers
        </a>
       </li>
      </ul>
     </li>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="archai.supergraph.models.html">
       Models
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-22" name="toctree-checkbox-22" type="checkbox"/>
      <label for="toctree-checkbox-22">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4">
        <a class="reference internal" href="archai.supergraph.models.shakeshake.html">
         ShakeShake
        </a>
       </li>
      </ul>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="archai.supergraph.nas.html">
       Neural Architecture Search
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="archai.supergraph.utils.html">
       Utilities
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="archai.trainers.html">
     Trainers
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-23" name="toctree-checkbox-23" type="checkbox"/>
    <label for="toctree-checkbox-23">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="archai.trainers.cv.html">
       Computer Vision
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="archai.trainers.nlp.html">
       Natural Language Processing
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../roadmap.html">
   Roadmap
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../changelog.html">
   Changelog
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/microsoft/archai"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/microsoft/archai/issues/new?title=Issue%20on%20page%20%2Freference/api/archai.quantization.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/microsoft/archai/edit/master/reference/api/archai.quantization.rst"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Edit this page"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="headerbtn__text-container">suggest edit</span>
</a>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Sections
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#module-archai.quantization.modules">
   Modules
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#module-archai.quantization.observers">
   Observers
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#module-archai.quantization.quantizers">
   Quantizers
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#module-archai.quantization.ptq">
   Post-Training Quantization (PTQ)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#module-archai.quantization.qat">
   Quantization-Aware Training (QAT)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#module-archai.quantization.mixed_qat">
   Mixed-QAT
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#module-archai.quantization.quantization_utils">
   Quantization (Utilities)
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Quantization</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Sections </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#module-archai.quantization.modules">
   Modules
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#module-archai.quantization.observers">
   Observers
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#module-archai.quantization.quantizers">
   Quantizers
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#module-archai.quantization.ptq">
   Post-Training Quantization (PTQ)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#module-archai.quantization.qat">
   Quantization-Aware Training (QAT)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#module-archai.quantization.mixed_qat">
   Mixed-QAT
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#module-archai.quantization.quantization_utils">
   Quantization (Utilities)
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section id="quantization">
<h1>Quantization<a class="headerlink" href="#quantization" title="Permalink to this headline">#</a></h1>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="archai.quantization.nlp.html">Natural Language Processing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="archai.quantization.nlp.html#module-archai.quantization.nlp.modules">Modules</a></li>
</ul>
</li>
</ul>
</div>
<section id="module-archai.quantization.modules">
<span id="modules"></span><h2>Modules<a class="headerlink" href="#module-archai.quantization.modules" title="Permalink to this headline">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="archai.quantization.modules.FakeQuantEmbedding">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">archai.quantization.modules.</span></span><span class="sig-name descname"><span class="pre">FakeQuantEmbedding</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/archai/quantization/modules.html#FakeQuantEmbedding"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#archai.quantization.modules.FakeQuantEmbedding" title="Permalink to this definition">#</a></dt>
<dd><p>Translate a torch-based Embedding layer into a QAT-ready Embedding layer.</p>
<dl class="py property">
<dt class="sig sig-object py" id="archai.quantization.modules.FakeQuantEmbedding.fake_quant_weight">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">fake_quant_weight</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">torch.Tensor</span></em><a class="headerlink" href="#archai.quantization.modules.FakeQuantEmbedding.fake_quant_weight" title="Permalink to this definition">#</a></dt>
<dd><p>Return a fake quantization over the weight matrix.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="archai.quantization.modules.FakeQuantEmbedding.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="../../_modules/archai/quantization/modules.html#FakeQuantEmbedding.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#archai.quantization.modules.FakeQuantEmbedding.forward" title="Permalink to this definition">#</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="archai.quantization.modules.FakeQuantEmbedding.from_float">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">from_float</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mod</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.nn.modules.module.Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">qconfig</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.nn.modules.module.Module</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#archai.quantization.modules.FakeQuantEmbedding" title="archai.quantization.modules.FakeQuantEmbedding"><span class="pre">archai.quantization.modules.FakeQuantEmbedding</span></a></span></span><a class="reference internal" href="../../_modules/archai/quantization/modules.html#FakeQuantEmbedding.from_float"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#archai.quantization.modules.FakeQuantEmbedding.from_float" title="Permalink to this definition">#</a></dt>
<dd><p>Map module from float to QAT-ready.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>mod</strong> – Module to be mapped.</p></li>
<li><p><strong>qconfig</strong> – Quantization configuration.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>QAT-ready module.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="archai.quantization.modules.FakeQuantEmbedding.to_float">
<span class="sig-name descname"><span class="pre">to_float</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.nn.modules.module.Module</span></span></span><a class="reference internal" href="../../_modules/archai/quantization/modules.html#FakeQuantEmbedding.to_float"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#archai.quantization.modules.FakeQuantEmbedding.to_float" title="Permalink to this definition">#</a></dt>
<dd><p>Map module from QAT-ready to float.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Float-based module.</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="archai.quantization.modules.FakeQuantEmbedding.num_embeddings">
<span class="sig-name descname"><span class="pre">num_embeddings</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#archai.quantization.modules.FakeQuantEmbedding.num_embeddings" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="archai.quantization.modules.FakeQuantEmbedding.embedding_dim">
<span class="sig-name descname"><span class="pre">embedding_dim</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#archai.quantization.modules.FakeQuantEmbedding.embedding_dim" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="archai.quantization.modules.FakeQuantEmbedding.padding_idx">
<span class="sig-name descname"><span class="pre">padding_idx</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#archai.quantization.modules.FakeQuantEmbedding.padding_idx" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="archai.quantization.modules.FakeQuantEmbedding.max_norm">
<span class="sig-name descname"><span class="pre">max_norm</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#archai.quantization.modules.FakeQuantEmbedding.max_norm" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="archai.quantization.modules.FakeQuantEmbedding.norm_type">
<span class="sig-name descname"><span class="pre">norm_type</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><a class="headerlink" href="#archai.quantization.modules.FakeQuantEmbedding.norm_type" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="archai.quantization.modules.FakeQuantEmbedding.scale_grad_by_freq">
<span class="sig-name descname"><span class="pre">scale_grad_by_freq</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#archai.quantization.modules.FakeQuantEmbedding.scale_grad_by_freq" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="archai.quantization.modules.FakeQuantEmbedding.weight">
<span class="sig-name descname"><span class="pre">weight</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">torch.Tensor</span></em><a class="headerlink" href="#archai.quantization.modules.FakeQuantEmbedding.weight" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="archai.quantization.modules.FakeQuantEmbedding.sparse">
<span class="sig-name descname"><span class="pre">sparse</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#archai.quantization.modules.FakeQuantEmbedding.sparse" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="archai.quantization.modules.FakeQuantEmbeddingForOnnx">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">archai.quantization.modules.</span></span><span class="sig-name descname"><span class="pre">FakeQuantEmbeddingForOnnx</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/archai/quantization/modules.html#FakeQuantEmbeddingForOnnx"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#archai.quantization.modules.FakeQuantEmbeddingForOnnx" title="Permalink to this definition">#</a></dt>
<dd><p>Allow a QAT-ready Embedding layer to be exported with ONNX.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="archai.quantization.modules.FakeQuantEmbeddingForOnnx.num_embeddings">
<span class="sig-name descname"><span class="pre">num_embeddings</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#archai.quantization.modules.FakeQuantEmbeddingForOnnx.num_embeddings" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="archai.quantization.modules.FakeQuantEmbeddingForOnnx.embedding_dim">
<span class="sig-name descname"><span class="pre">embedding_dim</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#archai.quantization.modules.FakeQuantEmbeddingForOnnx.embedding_dim" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="archai.quantization.modules.FakeQuantEmbeddingForOnnx.padding_idx">
<span class="sig-name descname"><span class="pre">padding_idx</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#archai.quantization.modules.FakeQuantEmbeddingForOnnx.padding_idx" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="archai.quantization.modules.FakeQuantEmbeddingForOnnx.max_norm">
<span class="sig-name descname"><span class="pre">max_norm</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#archai.quantization.modules.FakeQuantEmbeddingForOnnx.max_norm" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="archai.quantization.modules.FakeQuantEmbeddingForOnnx.norm_type">
<span class="sig-name descname"><span class="pre">norm_type</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><a class="headerlink" href="#archai.quantization.modules.FakeQuantEmbeddingForOnnx.norm_type" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="archai.quantization.modules.FakeQuantEmbeddingForOnnx.scale_grad_by_freq">
<span class="sig-name descname"><span class="pre">scale_grad_by_freq</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#archai.quantization.modules.FakeQuantEmbeddingForOnnx.scale_grad_by_freq" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="archai.quantization.modules.FakeQuantEmbeddingForOnnx.weight">
<span class="sig-name descname"><span class="pre">weight</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">torch.Tensor</span></em><a class="headerlink" href="#archai.quantization.modules.FakeQuantEmbeddingForOnnx.weight" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="archai.quantization.modules.FakeQuantEmbeddingForOnnx.sparse">
<span class="sig-name descname"><span class="pre">sparse</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#archai.quantization.modules.FakeQuantEmbeddingForOnnx.sparse" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="archai.quantization.modules.FakeQuantEmbeddingForOnnx.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#archai.quantization.modules.FakeQuantEmbeddingForOnnx.training" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="archai.quantization.modules.FakeDynamicQuantLinear">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">archai.quantization.modules.</span></span><span class="sig-name descname"><span class="pre">FakeDynamicQuantLinear</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dynamic_weight</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation_reduce_range</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bits</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">onnx_compatible</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">qconfig</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.nn.modules.module.Module</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/archai/quantization/modules.html#FakeDynamicQuantLinear"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#archai.quantization.modules.FakeDynamicQuantLinear" title="Permalink to this definition">#</a></dt>
<dd><p>Translate a torch-based Linear layer into a QAT-ready Linear layer.</p>
<dl class="py property">
<dt class="sig sig-object py" id="archai.quantization.modules.FakeDynamicQuantLinear.fake_quant_weight">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">fake_quant_weight</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">torch.Tensor</span></em><a class="headerlink" href="#archai.quantization.modules.FakeDynamicQuantLinear.fake_quant_weight" title="Permalink to this definition">#</a></dt>
<dd><p>Return a fake quantization over the weight matrix.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="archai.quantization.modules.FakeDynamicQuantLinear.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="../../_modules/archai/quantization/modules.html#FakeDynamicQuantLinear.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#archai.quantization.modules.FakeDynamicQuantLinear.forward" title="Permalink to this definition">#</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="archai.quantization.modules.FakeDynamicQuantLinear.from_float">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">from_float</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mod</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.nn.modules.module.Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">qconfig</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.nn.modules.module.Module</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation_reduce_range</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#archai.quantization.modules.FakeDynamicQuantLinear" title="archai.quantization.modules.FakeDynamicQuantLinear"><span class="pre">archai.quantization.modules.FakeDynamicQuantLinear</span></a></span></span><a class="reference internal" href="../../_modules/archai/quantization/modules.html#FakeDynamicQuantLinear.from_float"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#archai.quantization.modules.FakeDynamicQuantLinear.from_float" title="Permalink to this definition">#</a></dt>
<dd><p>Map module from float to QAT-ready.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>mod</strong> – Module to be mapped.</p></li>
<li><p><strong>qconfig</strong> – Quantization configuration.</p></li>
<li><p><strong>activation_reduce_range</strong> – Whether to reduce the range of activations.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>QAT-ready module.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="archai.quantization.modules.FakeDynamicQuantLinear.to_float">
<span class="sig-name descname"><span class="pre">to_float</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.nn.modules.module.Module</span></span></span><a class="reference internal" href="../../_modules/archai/quantization/modules.html#FakeDynamicQuantLinear.to_float"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#archai.quantization.modules.FakeDynamicQuantLinear.to_float" title="Permalink to this definition">#</a></dt>
<dd><p>Map module from QAT-ready to float.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Float-based module.</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="archai.quantization.modules.FakeDynamicQuantLinear.in_features">
<span class="sig-name descname"><span class="pre">in_features</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#archai.quantization.modules.FakeDynamicQuantLinear.in_features" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="archai.quantization.modules.FakeDynamicQuantLinear.out_features">
<span class="sig-name descname"><span class="pre">out_features</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#archai.quantization.modules.FakeDynamicQuantLinear.out_features" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="archai.quantization.modules.FakeDynamicQuantLinear.weight">
<span class="sig-name descname"><span class="pre">weight</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">torch.Tensor</span></em><a class="headerlink" href="#archai.quantization.modules.FakeDynamicQuantLinear.weight" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="archai.quantization.modules.FakeDynamicQuantLinearForOnnx">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">archai.quantization.modules.</span></span><span class="sig-name descname"><span class="pre">FakeDynamicQuantLinearForOnnx</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/archai/quantization/modules.html#FakeDynamicQuantLinearForOnnx"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#archai.quantization.modules.FakeDynamicQuantLinearForOnnx" title="Permalink to this definition">#</a></dt>
<dd><p>Allow a QAT-ready Linear layer to be exported with ONNX.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="archai.quantization.modules.FakeDynamicQuantLinearForOnnx.in_features">
<span class="sig-name descname"><span class="pre">in_features</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#archai.quantization.modules.FakeDynamicQuantLinearForOnnx.in_features" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="archai.quantization.modules.FakeDynamicQuantLinearForOnnx.out_features">
<span class="sig-name descname"><span class="pre">out_features</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#archai.quantization.modules.FakeDynamicQuantLinearForOnnx.out_features" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="archai.quantization.modules.FakeDynamicQuantLinearForOnnx.weight">
<span class="sig-name descname"><span class="pre">weight</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">torch.Tensor</span></em><a class="headerlink" href="#archai.quantization.modules.FakeDynamicQuantLinearForOnnx.weight" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="archai.quantization.modules.FakeDynamicQuantLinearForOnnx.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#archai.quantization.modules.FakeDynamicQuantLinearForOnnx.training" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="archai.quantization.modules.FakeDynamicQuantConv1d">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">archai.quantization.modules.</span></span><span class="sig-name descname"><span class="pre">FakeDynamicQuantConv1d</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dynamic_weight</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation_reduce_range</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bits</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">onnx_compatible</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">qconfig</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.nn.modules.module.Module</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/archai/quantization/modules.html#FakeDynamicQuantConv1d"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#archai.quantization.modules.FakeDynamicQuantConv1d" title="Permalink to this definition">#</a></dt>
<dd><p>Translate a torch-based Conv1d layer into a QAT-ready Conv1d layer.</p>
<dl class="py property">
<dt class="sig sig-object py" id="archai.quantization.modules.FakeDynamicQuantConv1d.fake_quant_weight">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">fake_quant_weight</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">torch.Tensor</span></em><a class="headerlink" href="#archai.quantization.modules.FakeDynamicQuantConv1d.fake_quant_weight" title="Permalink to this definition">#</a></dt>
<dd><p>Return a fake quantization over the weight matrix.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="archai.quantization.modules.FakeDynamicQuantConv1d.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="../../_modules/archai/quantization/modules.html#FakeDynamicQuantConv1d.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#archai.quantization.modules.FakeDynamicQuantConv1d.forward" title="Permalink to this definition">#</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="archai.quantization.modules.FakeDynamicQuantConv1d.from_float">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">from_float</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mod</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.nn.modules.module.Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">qconfig</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.nn.modules.module.Module</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation_reduce_range</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#archai.quantization.modules.FakeDynamicQuantConv1d" title="archai.quantization.modules.FakeDynamicQuantConv1d"><span class="pre">archai.quantization.modules.FakeDynamicQuantConv1d</span></a></span></span><a class="reference internal" href="../../_modules/archai/quantization/modules.html#FakeDynamicQuantConv1d.from_float"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#archai.quantization.modules.FakeDynamicQuantConv1d.from_float" title="Permalink to this definition">#</a></dt>
<dd><p>Map module from float to QAT-ready.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>mod</strong> – Module to be mapped.</p></li>
<li><p><strong>qconfig</strong> – Quantization configuration.</p></li>
<li><p><strong>activation_reduce_range</strong> – Whether to reduce the range of activations.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>QAT-ready module.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="archai.quantization.modules.FakeDynamicQuantConv1d.to_float">
<span class="sig-name descname"><span class="pre">to_float</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.nn.modules.module.Module</span></span></span><a class="reference internal" href="../../_modules/archai/quantization/modules.html#FakeDynamicQuantConv1d.to_float"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#archai.quantization.modules.FakeDynamicQuantConv1d.to_float" title="Permalink to this definition">#</a></dt>
<dd><p>Map module from QAT-ready to float.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Float-based module.</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="archai.quantization.modules.FakeDynamicQuantConv1d.bias">
<span class="sig-name descname"><span class="pre">bias</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#archai.quantization.modules.FakeDynamicQuantConv1d.bias" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="archai.quantization.modules.FakeDynamicQuantConv1d.in_channels">
<span class="sig-name descname"><span class="pre">in_channels</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#archai.quantization.modules.FakeDynamicQuantConv1d.in_channels" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="archai.quantization.modules.FakeDynamicQuantConv1d.out_channels">
<span class="sig-name descname"><span class="pre">out_channels</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#archai.quantization.modules.FakeDynamicQuantConv1d.out_channels" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="archai.quantization.modules.FakeDynamicQuantConv1d.kernel_size">
<span class="sig-name descname"><span class="pre">kernel_size</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#archai.quantization.modules.FakeDynamicQuantConv1d.kernel_size" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="archai.quantization.modules.FakeDynamicQuantConv1d.stride">
<span class="sig-name descname"><span class="pre">stride</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#archai.quantization.modules.FakeDynamicQuantConv1d.stride" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="archai.quantization.modules.FakeDynamicQuantConv1d.padding">
<span class="sig-name descname"><span class="pre">padding</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#archai.quantization.modules.FakeDynamicQuantConv1d.padding" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="archai.quantization.modules.FakeDynamicQuantConv1d.dilation">
<span class="sig-name descname"><span class="pre">dilation</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#archai.quantization.modules.FakeDynamicQuantConv1d.dilation" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="archai.quantization.modules.FakeDynamicQuantConv1d.transposed">
<span class="sig-name descname"><span class="pre">transposed</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#archai.quantization.modules.FakeDynamicQuantConv1d.transposed" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="archai.quantization.modules.FakeDynamicQuantConv1d.output_padding">
<span class="sig-name descname"><span class="pre">output_padding</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#archai.quantization.modules.FakeDynamicQuantConv1d.output_padding" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="archai.quantization.modules.FakeDynamicQuantConv1d.groups">
<span class="sig-name descname"><span class="pre">groups</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#archai.quantization.modules.FakeDynamicQuantConv1d.groups" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="archai.quantization.modules.FakeDynamicQuantConv1d.padding_mode">
<span class="sig-name descname"><span class="pre">padding_mode</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">str</span></em><a class="headerlink" href="#archai.quantization.modules.FakeDynamicQuantConv1d.padding_mode" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="archai.quantization.modules.FakeDynamicQuantConv1d.weight">
<span class="sig-name descname"><span class="pre">weight</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">torch.Tensor</span></em><a class="headerlink" href="#archai.quantization.modules.FakeDynamicQuantConv1d.weight" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="archai.quantization.modules.FakeDynamicQuantConv1dForOnnx">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">archai.quantization.modules.</span></span><span class="sig-name descname"><span class="pre">FakeDynamicQuantConv1dForOnnx</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/archai/quantization/modules.html#FakeDynamicQuantConv1dForOnnx"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#archai.quantization.modules.FakeDynamicQuantConv1dForOnnx" title="Permalink to this definition">#</a></dt>
<dd><p>Allow a QAT-ready Conv1d layer to be exported with ONNX.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="archai.quantization.modules.FakeDynamicQuantConv1dForOnnx.bias">
<span class="sig-name descname"><span class="pre">bias</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#archai.quantization.modules.FakeDynamicQuantConv1dForOnnx.bias" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="archai.quantization.modules.FakeDynamicQuantConv1dForOnnx.in_channels">
<span class="sig-name descname"><span class="pre">in_channels</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#archai.quantization.modules.FakeDynamicQuantConv1dForOnnx.in_channels" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="archai.quantization.modules.FakeDynamicQuantConv1dForOnnx.out_channels">
<span class="sig-name descname"><span class="pre">out_channels</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#archai.quantization.modules.FakeDynamicQuantConv1dForOnnx.out_channels" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="archai.quantization.modules.FakeDynamicQuantConv1dForOnnx.kernel_size">
<span class="sig-name descname"><span class="pre">kernel_size</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#archai.quantization.modules.FakeDynamicQuantConv1dForOnnx.kernel_size" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="archai.quantization.modules.FakeDynamicQuantConv1dForOnnx.stride">
<span class="sig-name descname"><span class="pre">stride</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#archai.quantization.modules.FakeDynamicQuantConv1dForOnnx.stride" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="archai.quantization.modules.FakeDynamicQuantConv1dForOnnx.padding">
<span class="sig-name descname"><span class="pre">padding</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#archai.quantization.modules.FakeDynamicQuantConv1dForOnnx.padding" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="archai.quantization.modules.FakeDynamicQuantConv1dForOnnx.dilation">
<span class="sig-name descname"><span class="pre">dilation</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#archai.quantization.modules.FakeDynamicQuantConv1dForOnnx.dilation" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="archai.quantization.modules.FakeDynamicQuantConv1dForOnnx.transposed">
<span class="sig-name descname"><span class="pre">transposed</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#archai.quantization.modules.FakeDynamicQuantConv1dForOnnx.transposed" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="archai.quantization.modules.FakeDynamicQuantConv1dForOnnx.output_padding">
<span class="sig-name descname"><span class="pre">output_padding</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#archai.quantization.modules.FakeDynamicQuantConv1dForOnnx.output_padding" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="archai.quantization.modules.FakeDynamicQuantConv1dForOnnx.groups">
<span class="sig-name descname"><span class="pre">groups</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#archai.quantization.modules.FakeDynamicQuantConv1dForOnnx.groups" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="archai.quantization.modules.FakeDynamicQuantConv1dForOnnx.padding_mode">
<span class="sig-name descname"><span class="pre">padding_mode</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">str</span></em><a class="headerlink" href="#archai.quantization.modules.FakeDynamicQuantConv1dForOnnx.padding_mode" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="archai.quantization.modules.FakeDynamicQuantConv1dForOnnx.weight">
<span class="sig-name descname"><span class="pre">weight</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">torch.Tensor</span></em><a class="headerlink" href="#archai.quantization.modules.FakeDynamicQuantConv1dForOnnx.weight" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="archai.quantization.modules.FakeDynamicQuantConv1dForOnnx.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#archai.quantization.modules.FakeDynamicQuantConv1dForOnnx.training" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="module-archai.quantization.observers">
<span id="observers"></span><h2>Observers<a class="headerlink" href="#module-archai.quantization.observers" title="Permalink to this headline">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="archai.quantization.observers.OnnxDynamicObserver">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">archai.quantization.observers.</span></span><span class="sig-name descname"><span class="pre">OnnxDynamicObserver</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/archai/quantization/observers.html#OnnxDynamicObserver"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#archai.quantization.observers.OnnxDynamicObserver" title="Permalink to this definition">#</a></dt>
<dd><p>DynamicObserver that is compliant with ONNX-based graphs.</p>
<p>This class can be used to perform symmetric or assymetric quantization, depending on the
<cite>dtype</cite> provided. <cite>qint8</cite> is usually used for symmetric quantization, while <cite>quint8</cite> is
used for assymetric quantization.</p>
<dl class="py method">
<dt class="sig sig-object py" id="archai.quantization.observers.OnnxDynamicObserver.calculate_qparams">
<span class="sig-name descname"><span class="pre">calculate_qparams</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../../_modules/archai/quantization/observers.html#OnnxDynamicObserver.calculate_qparams"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#archai.quantization.observers.OnnxDynamicObserver.calculate_qparams" title="Permalink to this definition">#</a></dt>
<dd><p>Calculate the quantization parameters.</p>
</dd></dl>

</dd></dl>

</section>
<section id="module-archai.quantization.quantizers">
<span id="quantizers"></span><h2>Quantizers<a class="headerlink" href="#module-archai.quantization.quantizers" title="Permalink to this headline">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="archai.quantization.quantizers.FakeDynamicQuant">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">archai.quantization.quantizers.</span></span><span class="sig-name descname"><span class="pre">FakeDynamicQuant</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">reduce_range</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.dtype</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">torch.quint8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bits</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">onnx_compatible</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/archai/quantization/quantizers.html#FakeDynamicQuant"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#archai.quantization.quantizers.FakeDynamicQuant" title="Permalink to this definition">#</a></dt>
<dd><p>Fake dynamic quantizer to allow for scale/zero point calculation during Quantization-Aware Training.</p>
<p>This class allows inserting a fake dynamic quantization operator in a PyTorch model,
in order to calculate scale and zero point values that can be used to quantize the
model during training. The operator can be customized to use different quantization types
(quint8 or qint8) and bit widths, and it can be made compatible with ONNX.</p>
<p>Note: This module is only meant to be used during training, and should not be present
in the final, deployed model.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="archai.quantization.quantizers.FakeDynamicQuant.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#archai.quantization.quantizers.FakeDynamicQuant.training" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="archai.quantization.quantizers.FakeDynamicQuant.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="../../_modules/archai/quantization/quantizers.html#FakeDynamicQuant.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#archai.quantization.quantizers.FakeDynamicQuant.forward" title="Permalink to this definition">#</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

</section>
<section id="module-archai.quantization.ptq">
<span id="post-training-quantization-ptq"></span><h2>Post-Training Quantization (PTQ)<a class="headerlink" href="#module-archai.quantization.ptq" title="Permalink to this headline">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="archai.quantization.ptq.GemmQuant">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">archai.quantization.ptq.</span></span><span class="sig-name descname"><span class="pre">GemmQuant</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">onnx_quantizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">onnxruntime.quantization.onnx_quantizer.ONNXQuantizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">onnx_node</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">onnx.onnx_ml_pb2.NodeProto</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/archai/quantization/ptq.html#GemmQuant"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#archai.quantization.ptq.GemmQuant" title="Permalink to this definition">#</a></dt>
<dd><p>Quantized version of the Gemm operator.</p>
<dl class="py method">
<dt class="sig sig-object py" id="archai.quantization.ptq.GemmQuant.quantize">
<span class="sig-name descname"><span class="pre">quantize</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../../_modules/archai/quantization/ptq.html#GemmQuant.quantize"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#archai.quantization.ptq.GemmQuant.quantize" title="Permalink to this definition">#</a></dt>
<dd><p>Quantize a Gemm node into QGemm.</p>
<p>This method replaces the original <cite>Gemm</cite> node with a <cite>QGemm</cite> node, which is a quantized
version of the <cite>Gemm</cite> operator. It also adds a <cite>Cast</cite> node to cast the output of <cite>QGemm</cite>
to float, and an <cite>Add</cite> node to sum the remaining bias to the <cite>Gemm</cite> output.</p>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="archai.quantization.ptq.add_new_quant_operators">
<span class="sig-prename descclassname"><span class="pre">archai.quantization.ptq.</span></span><span class="sig-name descname"><span class="pre">add_new_quant_operators</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../../_modules/archai/quantization/ptq.html#add_new_quant_operators"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#archai.quantization.ptq.add_new_quant_operators" title="Permalink to this definition">#</a></dt>
<dd><p>Add support for new quantization operators by changing
internal onnxruntime registry dictionaries.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="archai.quantization.ptq.dynamic_quantization_onnx">
<span class="sig-prename descclassname"><span class="pre">archai.quantization.ptq.</span></span><span class="sig-name descname"><span class="pre">dynamic_quantization_onnx</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">onnx_model_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="reference internal" href="../../_modules/archai/quantization/ptq.html#dynamic_quantization_onnx"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#archai.quantization.ptq.dynamic_quantization_onnx" title="Permalink to this definition">#</a></dt>
<dd><p>Perform dynamic quantization on an ONNX model.</p>
<p>The quantized model is saved to a new file with “-int8” appended
to the original file name.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>onnx_model_path</strong> – Path to the ONNX model to be quantized.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Path to the dynamic quantized ONNX model.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="archai.quantization.ptq.dynamic_quantization_torch">
<span class="sig-prename descclassname"><span class="pre">archai.quantization.ptq.</span></span><span class="sig-name descname"><span class="pre">dynamic_quantization_torch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.nn.modules.module.Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">embedding_layers</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">['word_emb',</span> <span class="pre">'transformer.wpe',</span> <span class="pre">'transformer.wte']</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../../_modules/archai/quantization/ptq.html#dynamic_quantization_torch"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#archai.quantization.ptq.dynamic_quantization_torch" title="Permalink to this definition">#</a></dt>
<dd><p>Perform dynamic quantization on a PyTorch model.</p>
<p>This function performs dynamic quantization on the input PyTorch model, including
any specified embedding layers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> – PyTorch model to be quantized.</p></li>
<li><p><strong>embedding_layers</strong> – List of string-based identifiers of embedding layers to be quantized.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-archai.quantization.qat">
<span id="quantization-aware-training-qat"></span><h2>Quantization-Aware Training (QAT)<a class="headerlink" href="#module-archai.quantization.qat" title="Permalink to this headline">#</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="archai.quantization.qat.qat_to_float_modules">
<span class="sig-prename descclassname"><span class="pre">archai.quantization.qat.</span></span><span class="sig-name descname"><span class="pre">qat_to_float_modules</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.nn.modules.module.Module</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../../_modules/archai/quantization/qat.html#qat_to_float_modules"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#archai.quantization.qat.qat_to_float_modules" title="Permalink to this definition">#</a></dt>
<dd><p>Convert QAT-ready modules to float-based modules.</p>
<p>This function converts all QAT-ready modules in the input model to float-based modules.
It does this recursively, so all sub-modules within the input model will also be
converted if applicable.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>model</strong> – QAT-ready module to be converted.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="archai.quantization.qat.float_to_qat_modules">
<span class="sig-prename descclassname"><span class="pre">archai.quantization.qat.</span></span><span class="sig-name descname"><span class="pre">float_to_qat_modules</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model:</span> <span class="pre">torch.nn.modules.module.Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">module_mapping:</span> <span class="pre">Optional[Dict[torch.nn.modules.module.Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">torch.nn.modules.module.Module]]</span> <span class="pre">=</span> <span class="pre">{&lt;class</span> <span class="pre">'torch.nn.modules.conv.Conv1d'&gt;:</span> <span class="pre">&lt;class</span> <span class="pre">'archai.quantization.modules.FakeDynamicQuantConv1d'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">&lt;class</span> <span class="pre">'torch.nn.modules.linear.Linear'&gt;:</span> <span class="pre">&lt;class</span> <span class="pre">'archai.quantization.modules.FakeDynamicQuantLinear'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">&lt;class</span> <span class="pre">'torch.nn.modules.sparse.Embedding'&gt;:</span> <span class="pre">&lt;class</span> <span class="pre">'archai.quantization.modules.FakeQuantEmbedding'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">&lt;class</span> <span class="pre">'transformers.pytorch_utils.Conv1D'&gt;:</span> <span class="pre">&lt;class</span> <span class="pre">'archai.quantization.nlp.modules.FakeDynamicQuantHFConv1D'&gt;}</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">qconfig:</span> <span class="pre">Optional[Dict[torch.nn.modules.module.Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Any]]</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">**kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../../_modules/archai/quantization/qat.html#float_to_qat_modules"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#archai.quantization.qat.float_to_qat_modules" title="Permalink to this definition">#</a></dt>
<dd><p>Convert float-based modules to QAT-ready modules.</p>
<p>This function converts all float-based modules in the input model to QAT-ready
modules using the provided module mapping. It does this recursively, so all sub-modules
within the input model will also be converted if applicable.</p>
<p>A quantization configuration can also be supplied.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> – Float-based module to be converted.</p></li>
<li><p><strong>module_mapping</strong> – Maps between float and QAT-ready modules.</p></li>
<li><p><strong>qconfig</strong> – Quantization configuration to be used for the conversion.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="archai.quantization.qat.prepare_with_qat">
<span class="sig-prename descclassname"><span class="pre">archai.quantization.qat.</span></span><span class="sig-name descname"><span class="pre">prepare_with_qat</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.nn.modules.module.Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inplace</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">onnx_compatible</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">backend</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'qnnpack'</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.nn.modules.module.Module</span></span></span><a class="reference internal" href="../../_modules/archai/quantization/qat.html#prepare_with_qat"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#archai.quantization.qat.prepare_with_qat" title="Permalink to this definition">#</a></dt>
<dd><p>Prepare a float-based PyTorch model for quantization-aware training (QAT).</p>
<p>This function modifies the input model in place by inserting
QAT-based modules and configurations.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> – Float-based PyTorch module to be prepared for QAT.</p></li>
<li><p><strong>inplace</strong> – Whether the prepared QAT model should replace the original model.</p></li>
<li><p><strong>onnx_compatible</strong> – Whether the prepared QAT model should be compatible with ONNX.</p></li>
<li><p><strong>backend</strong> – Quantization backend to be used.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The input model, modified in place (or not) to be ready for QAT.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-archai.quantization.mixed_qat">
<span id="mixed-qat"></span><h2>Mixed-QAT<a class="headerlink" href="#module-archai.quantization.mixed_qat" title="Permalink to this headline">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="archai.quantization.mixed_qat.MixedQAT">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">archai.quantization.mixed_qat.</span></span><span class="sig-name descname"><span class="pre">MixedQAT</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.nn.modules.module.Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">qat_weight</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.2</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/archai/quantization/mixed_qat.html#MixedQAT"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#archai.quantization.mixed_qat.MixedQAT" title="Permalink to this definition">#</a></dt>
<dd><p>Mixed QAT (Quantization-Aware Training) model, which can be fine-tuned
using a linear combination of regular and QAT losses.</p>
<dl class="py method">
<dt class="sig sig-object py" id="archai.quantization.mixed_qat.MixedQAT.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_ids</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.LongTensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.LongTensor</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../../_modules/archai/quantization/mixed_qat.html#MixedQAT.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#archai.quantization.mixed_qat.MixedQAT.forward" title="Permalink to this definition">#</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="archai.quantization.mixed_qat.MixedQAT.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#archai.quantization.mixed_qat.MixedQAT.training" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="module-archai.quantization.quantization_utils">
<span id="quantization-utilities"></span><h2>Quantization (Utilities)<a class="headerlink" href="#module-archai.quantization.quantization_utils" title="Permalink to this headline">#</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="archai.quantization.quantization_utils.rgetattr">
<span class="sig-prename descclassname"><span class="pre">archai.quantization.quantization_utils.</span></span><span class="sig-name descname"><span class="pre">rgetattr</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">obj</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attr</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Any</span></span></span><a class="reference internal" href="../../_modules/archai/quantization/quantization_utils.html#rgetattr"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#archai.quantization.quantization_utils.rgetattr" title="Permalink to this definition">#</a></dt>
<dd><p>Recursively get an attribute from an object.</p>
<p>This function allows accessing nested attributes by separating each level with a dot (e.g., “attr1.attr2.attr3”).
If any attribute along the chain does not exist, the function returns the default value
specified in the <cite>*args</cite> parameter.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>obj</strong> – Object from which the attribute will be retrieved.</p></li>
<li><p><strong>attr</strong> – Name of the attribute to be retrieved, with each level separated by a dot.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Attribute from the object.</p>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">obj</span> <span class="o">=</span> <span class="n">MyObject</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rgetattr</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="s2">&quot;attr1.attr2.attr3&quot;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="simple">
<dt>Reference:</dt><dd><p><a class="reference external" href="https://stackoverflow.com/questions/31174295/getattr-and-setattr-on-nested-subobjects-chained-properties/31174427#31174427">https://stackoverflow.com/questions/31174295/getattr-and-setattr-on-nested-subobjects-chained-properties/31174427#31174427</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="archai.quantization.quantization_utils.rsetattr">
<span class="sig-prename descclassname"><span class="pre">archai.quantization.quantization_utils.</span></span><span class="sig-name descname"><span class="pre">rsetattr</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">obj</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attr</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../../_modules/archai/quantization/quantization_utils.html#rsetattr"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#archai.quantization.quantization_utils.rsetattr" title="Permalink to this definition">#</a></dt>
<dd><p>Recursively set an attribute on an object.</p>
<p>This function allows setting nested attributes by separating each level with a dot (e.g., “attr1.attr2.attr3”).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>obj</strong> – Object on which the attribute will be set.</p></li>
<li><p><strong>attr</strong> – Name of the attribute to be set, with each level separated by a dot.</p></li>
<li><p><strong>value</strong> – New value for the attribute.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">obj</span> <span class="o">=</span> <span class="n">MyObject</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rsetattr</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="s2">&quot;attr1.attr2.attr3&quot;</span><span class="p">,</span> <span class="n">new_value</span><span class="p">)</span>
</pre></div>
</div>
<dl class="simple">
<dt>Reference:</dt><dd><p><a class="reference external" href="https://stackoverflow.com/questions/31174295/getattr-and-setattr-on-nested-subobjects-chained-properties/31174427#31174427">https://stackoverflow.com/questions/31174295/getattr-and-setattr-on-nested-subobjects-chained-properties/31174427#31174427</a></p>
</dd>
</dl>
</dd></dl>

</section>
</section>


              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="archai.onnx.optimization_utils.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Optimization Utilities</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="archai.quantization.nlp.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Natural Language Processing</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Microsoft<br/>
  
      &copy; Copyright 2023.<br/>
    Last updated on Feb 13, 2023.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>