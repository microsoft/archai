data:
  dataset:
    dataset_name: "wikitext"
    dataset_config_name: "wikitext-103-raw-v1"
    dataset_split: null
  grouped: false
tokenizer:
  tokenizer_path: "gpt2"
  model_max_length: 192
model:
  model_name: "clm-gpt2"
  model_type: "gpt2"
  vocab_size: 50258
  n_positions: 192
  n_ctx: 1024
  n_embd: 768
  n_layer: 12
  n_head: 12
  n_inner: null
  activation_function: "gelu"
  resid_pdrop: 0.1
  embd_pdrop: 0.1
  attn_pdrop: 0.1
  layer_norm_epsilon: 0.00001
  initializer_range: 0.02
  summary_type: "cls_index"
  summary_use_proj: true
  summary_activation: null
  summary_proj_to_labels: true
  summary_first_dropout: 0.1
  scale_attn_weights: true
  use_cache: true
  bos_token_id: 1
  eos_token_id: 2
  scale_attn_by_inverse_layer_idx: false
  reorder_and_upcast_attn: false
trainer:
  evaluation_strategy: "steps"
  per_device_train_batch_size: 16
  per_device_eval_batch_size: 16
  gradient_accumulation_steps: 2
  learning_rate: 5.0e-4
  weight_decay: 0.0
  max_grad_norm: 1.0
  max_steps: 100_000
  lr_scheduler_type: "cosine"
  warmup_steps: 1000
  logging_steps: 10
  save_steps: 1_000
  seed: 1234
  fp16: false
  eval_steps: 1_000
