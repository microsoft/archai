# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

collator:
  collator_name: "language-modelling"
  mlm: False
data:
  dataset:
    - dataset_disk: "/data/dataroot/dummy"
  num_proc: 128
  mapping_column_name: "code"
  encoded_dataset_path: "/data/codedatasets/only_python_the_stack_encoded_16384"
tokenizer:
  tokenizer_file: "Salesforce/codegen-350M-mono"
  pad_token: "<|endoftext|>"
  model_max_length: 16384
model:
  model_type: "codegen_sgconv"
  vocab_size: 50295
  n_positions: 16384
  n_embd: 768
  n_layer: 12
  n_head: 12
  n_inner: null
  rotary_dim: 32
  activation_function: "gelu_new"
  attn_pdrop: 0.0
  bos_token_id: 1
  embd_pdrop: 0.0
  eos_token_id: 2
  initializer_range: 0.02
  layer_norm_epsilon: 0.00001
  resid_pdrop: 0.0
  scale_attn_weights: true
  summary_activation: null
  summary_first_dropout: 0.1
  summary_proj_to_labels: true
  summary_type: "cls_index"
  summary_use_proj: true
  tie_word_embeddings: false
  use_cache: false
  sgconv_d_state: 64
  sgconv_channels: 1
  sgconv_kernel_dim: 128
  sgconv_decay_min: 2
  sgconv_decay_max: 2
trainer:
  optim: "adamw_torch"
  evaluation_strategy: "no"
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  gradient_accumulation_steps: 8
  learning_rate: 0.0018
  adam_beta1: 0.9
  adam_beta2: 0.95
  weight_decay: 0.1
  max_grad_norm: 1.0
  max_steps: 100
  lr_scheduler_type: "cosine"
  warmup_steps: 300
  logging_steps: 10
  save_steps: 10
  save_total_limit: 5
  eval_steps: 5
  seed: 1234
  fp16: true
  tf32: true