# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

collator:
  collator_name: "language-modelling"
  mlm: False
data:
  dataset:
    - dataset_disk: "/mnt/archai_nlp/datasets/dummy"
  num_proc: 128
  mapping_column_name: "code"
  encoded_dataset_path: "/mnt/archai_nlp/datasets/the_pile_30B_encoded_8192"
tokenizer:
  tokenizer_file: "Salesforce/codegen-350M-mono"
  pad_token: "<|endoftext|>"
  model_max_length: 8192
model:
  model_type: "codegen"
  vocab_size: 50295
  n_positions: 8192
  n_embd: 512
  n_layer: 5
  n_head: 8
  n_inner: null
  rotary_dim: 32
  activation_function: "gelu_new"
  attn_pdrop: 0.0
  bos_token_id: 1
  embd_pdrop: 0.0
  eos_token_id: 2
  initializer_range: 0.02
  layer_norm_epsilon: 0.00001
  resid_pdrop: 0.0
  scale_attn_weights: true
  summary_activation: null
  summary_first_dropout: 0.1
  summary_proj_to_labels: true
  summary_type: "cls_index"
  summary_use_proj: true
  tie_word_embeddings: false
  use_cache: false
trainer:
  optim: "adamw_torch"
  evaluation_strategy: "no"
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  gradient_accumulation_steps: 32
  learning_rate: 0.0018
  adam_beta1: 0.9
  adam_beta2: 0.95
  weight_decay: 0.1
  max_grad_norm: 1.0
  max_steps: 40000
  lr_scheduler_type: "cosine"
  warmup_steps: 3000
  logging_steps: 20
  save_steps: 2000
  save_total_limit: 5
  eval_steps: 6000
  seed: 1234
  fp16: true
  tf32: false