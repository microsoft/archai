{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp 1: Try various learning rates on pure SGConv model\n",
    "\n",
    "#### Details\n",
    "\n",
    "* dataset: python portion of the stack\n",
    "* context length 8k\n",
    "* 6 layers, 512 embedding dim, kernel dim 64, only sgconv layers\n",
    "* 50_000 training steps, each step 64 effective batch size\n",
    "* 70M parameters approx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 0.0018 2.1382687863159178\n",
      "lr: 0.0064 2.17441978805542\n",
      "lr: 0.0128 2.2805975857543945\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# experiment logs root directory\n",
    "exp_dir = \"/data/amlt_sgconv_standalone/\" \n",
    "\n",
    "exp_names = ['sgconv-ctx-8k-l-6-embed-512-k-64-lr-0.0018-pyst-8xV100', \n",
    "             'sgconv-ctx-8k-l-6-embed-512-k-64-lr-0.0064-pyst-8xV100', \n",
    "             'sgconv-ctx-8k-l-6-embed-512-k-64-lr-0.0128-pyst-8xV100']\n",
    "\n",
    "full_paths = [os.path.join(exp_dir, e, 'sgconv-python-stack-8k', 'codegen_sgconv', 'train_results.json')  for e in exp_names]\n",
    "\n",
    "train_results = []\n",
    "for p in full_paths:\n",
    "    with open(p, 'r') as f:\n",
    "        train_results.append(json.load(f))\n",
    "\n",
    "for lr, l in zip([0.0018, 0.0064, 0.0128], train_results):\n",
    "    print(f'lr: {lr} {l[\"train_loss\"]}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "of the three learning rates tried 0.0018 is the best so far. perhaps try more extreme learning rates and also fix batching to be 256 effective batch size as in Pile 30B \n",
    "\n",
    "after fixing batching to be effectively 256, and learning rates of \n",
    "* 1.0, \n",
    "* 0.1, \n",
    "* 0.01, \n",
    "* 0.001, \n",
    "  \n",
    "  0.01 and 0.001 are stable, higher values are unstable. 0.01 is a bit unstable but okay.\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Details:\n",
    "\n",
    "* dataset: 30B tokens from The Pile\n",
    "* context length 8k\n",
    "* 6 layers, 512 embedding dim, kernel dim 64, only sgconv layers\n",
    "* 40_000 training steps, each step 256 effective batch size\n",
    "* 70M parameters approx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 0.0018 3.4737164328575134\n",
      "lr: 0.0064 3.455818660545349\n",
      "lr: 0.0128 3.49161922454834\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# experiment logs root directory\n",
    "exp_dir = \"/data/amlt_sgconv_standalone/sgconv-ctx-8k-l-6-embed-512-k-64-p30-tune-8xV100\" \n",
    "\n",
    "exp_names = ['lr-0.0018', 'lr-0.0064', 'lr-0.0128']\n",
    "\n",
    "full_paths = [os.path.join(exp_dir, e, 'codegen_sgconv', 'train_results.json') for e in exp_names]\n",
    "\n",
    "train_results = []\n",
    "for p in full_paths:\n",
    "    with open(p, 'r') as f:\n",
    "        train_results.append(json.load(f))\n",
    "\n",
    "for lr, l in zip([0.0018, 0.0064, 0.0128], train_results):\n",
    "    print(f'lr: {lr} {l[\"train_loss\"]}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### of the learning rates tried so far, no clear winners"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Exp 2: Pure transformer model on 8k context length\n",
    "\n",
    "#### Details\n",
    "\n",
    "* dataset: python portion of the stack\n",
    "* context length 8k\n",
    "* 5 layers, 8 heads, 512 embedding dim, only self-attentin layers \n",
    "* 50_000 training steps, each step 64 effective batch size\n",
    "* 70M parameters approx.\n",
    "\n",
    "Only tried 0.0018 but was really unstable in both 8xV100 and 16xV100 mode."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Details\n",
    "\n",
    "* dataset: 30 B tokens from The PIle\n",
    "* context length 8k\n",
    "* 5 layers, 8 heads, 512 embedding dim, only self-attentin layers \n",
    "* 40_000 training steps, each step 256 effective batch size\n",
    "* 70M parameters approx.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "archai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15 (default, Nov  4 2022, 20:59:55) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2bc53cc0efc4c5832814526cd5c46d606e57193e6a401cefa48dbee83307ce77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
