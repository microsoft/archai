# Research Project: CodeGen with Different Attentions

## Abstract

This research project aims to explore alternative CodeGen-based models that utilize different attention mechanisms. The primary goal is to determine the effectiveness and efficiency of using Hard-Coded Attention, Convolutional Attention, and Structured Global Convolutions (SGConv) in CodeGen models. The results of this study could potentially lead to the development of more efficient and accurate CodeGen models.

## Installation

`pip install -r requirements.txt`

## Quick Start

`python run_experiment.py configs/clm-codegen-sgconv-sm.yaml`